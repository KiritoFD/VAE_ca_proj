import torch
import torch.nn as nn
import math

class GatedDifferentialBlock(nn.Module):
    """
    SA-Flow æ ¸å¿ƒç»„ä»¶ï¼šé—¨æ§å¾®åˆ†å—
    åˆ©ç”¨å¤§æ ¸å·ç§¯è¿‘ä¼¼å±€éƒ¨å¾®åˆ†æµï¼Œå®Œå…¨ä¿ç•™ç©ºé—´æ‹“æ‰‘ç»“æ„ã€‚
    """
    def __init__(self, dim, kernel_size=7):
        super().__init__()
        # 1. å±€éƒ¨å¾®åˆ†æµ (Local Differential Term)
        # Depthwise Conv: æä½å‚æ•°é‡ï¼Œæ•æ‰å±€éƒ¨çº¹ç†æµå‘ï¼Œä¿æŒæ‹“æ‰‘
        self.local_flow = nn.Conv2d(dim, dim, kernel_size=kernel_size, 
                                    padding=kernel_size//2, groups=dim)
        
        # 2. å…¨å±€é£æ ¼åŠ¿èƒ½ (Global Potential Term)
        # GroupNorm ä¿æŒç©ºé—´ç»“æ„ï¼Œä¸åƒ LayerNorm é‚£æ ·Flatten
        self.norm = nn.GroupNorm(32, dim) 
        self.style_proj = nn.Linear(dim, dim * 2) 
        
        # 3. æ··åˆä¸éçº¿æ€§ (Flow Mixing)
        # 1x1 Conv æ›¿ä»£ Linearï¼Œå®ç°é€šé“é—´çš„ä¿¡æ¯äº¤äº’
        self.proj_1 = nn.Conv2d(dim, dim * 2, 1) 
        self.proj_2 = nn.Conv2d(dim, dim, 1)
        self.act = nn.SiLU()
        
        # ğŸ”´ ç§»é™¤ self.scale - è®© GroupNorm å’Œ Residual è‡ªå·±å¹³è¡¡

    def forward(self, x, style_emb):
        # x: [B, C, H, W]
        shortcut = x
        
        # A. æ³¨å…¥å…¨å±€é£æ ¼ (AdaGN)
        # style_emb: [B, dim] -> [B, 2*dim]
        style_params = self.style_proj(style_emb)
        mu, sigma = style_params.chunk(2, dim=-1)
        
        # å¹¿æ’­åˆ°ç©ºé—´ç»´åº¦ [B, dim, 1, 1]
        mu = mu.unsqueeze(-1).unsqueeze(-1)
        sigma = sigma.unsqueeze(-1).unsqueeze(-1)
        
        # è°ƒåˆ¶ï¼šNormåè¿›è¡Œç¼©æ”¾å’Œå¹³ç§»
        x = self.norm(x) * (1 + sigma) + mu
        
        # B. å±€éƒ¨ç©ºé—´å»ºæ¨¡ (No Attention, just Large Kernel Conv)
        x = self.local_flow(x)
        
        # C. é—¨æ§æµä½“æ··åˆ (GLU)
        # æ¨¡æ‹Ÿæµä½“åŠ›å­¦ä¸­çš„éçº¿æ€§ç²˜æ»
        x_gate, x_val = self.proj_1(x).chunk(2, dim=1)
        x = self.act(x_gate) * x_val
        x = self.proj_2(x)
        
        # D. æ¬§æ‹‰ç§¯åˆ†æ­¥ (Residual Connection)
        # ğŸ”´ ç›´æ¥ç›¸åŠ ,ä¸ä¹˜æå°çš„ scale
        return shortcut + x


class SinusoidalTimeEmbedding(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
    
    def forward(self, t):
        device = t.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = t[:, None] * embeddings[None, :]
        embeddings = torch.cat([embeddings.sin(), embeddings.cos()], dim=-1)
        return embeddings


class SAFModel(nn.Module):
    """
    SA-Flow Architecture (Structure-Aware Flow)
    æ›¿ä»£åŸæœ‰çš„ Transformer æ¶æ„ï¼Œä¸“é—¨é’ˆå¯¹ Image-to-Image Mapping ä¼˜åŒ–ã€‚
    ä¿æŒäº†ä¸åŸ DiTModel ç›¸åŒçš„è¾“å…¥è¾“å‡ºæ¥å£ã€‚
    """
    def __init__(
        self,
        latent_channels=4,
        latent_size=64, # ä»…å ä½ï¼ŒSA-Flow ä¸ä¾èµ–å›ºå®šå°ºå¯¸
        hidden_dim=384,
        num_layers=12,
        num_styles=2,
        kernel_size=7,
        **kwargs # åæ‰ config ä¸­ä¸å†éœ€è¦çš„ transformer å‚æ•°
    ):
        super().__init__()
        self.in_channels = latent_channels * 2 # xt + x_content
        self.hidden_dim = hidden_dim
        
        # 1. é£æ ¼åµŒå…¥ (Style Embedding)
        # è¿™å°±æ˜¯æ–¹æ¡ˆä¸€é‡Œæåˆ°çš„â€œé£æ ¼èº«ä»½è¯â€
        self.style_embed = nn.Embedding(num_styles, hidden_dim)
        
        # 2. æ—¶é—´åµŒå…¥
        self.time_mlp = nn.Sequential(
            SinusoidalTimeEmbedding(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # 3. å…¥å£å±‚ (Stem)
        # ç›´æ¥åœ¨ Latent ç©ºé—´å·ç§¯ï¼Œä¿æŒ 2D ç»“æ„
        self.stem = nn.Conv2d(self.in_channels, hidden_dim, kernel_size=3, padding=1)
        
        # 4. æ ¸å¿ƒå¾®åˆ†æµå— (Differential Blocks)
        self.blocks = nn.ModuleList([
            GatedDifferentialBlock(hidden_dim, kernel_size=kernel_size)
            for _ in range(num_layers)
        ])
        
        # 5. å‡ºå£å±‚ (Final Velocity Prediction)
        self.final_norm = nn.GroupNorm(32, hidden_dim)
        self.final_conv = nn.Conv2d(hidden_dim, latent_channels, kernel_size=3, padding=1)
        
        # åˆå§‹åŒ–
        self.initialize_weights()

    def initialize_weights(self):
        # æœ€åä¸€å±‚åˆå§‹åŒ–ä¸ºé›¶ï¼Œä¿è¯åˆå§‹çŠ¶æ€ä¸‹æ¨¡å‹è¾“å‡ºæ¥è¿‘é›¶é€Ÿåº¦ï¼ˆæ’ç­‰æ˜ å°„ï¼‰
        nn.init.zeros_(self.final_conv.weight)
        nn.init.zeros_(self.final_conv.bias)
        
        # ğŸ”´ æ·»åŠ : æ˜¾å¼åˆå§‹åŒ–ä¸­é—´å±‚,ç¡®ä¿æ¢¯åº¦æµåŠ¨
        for m in self.modules():
            if isinstance(m, nn.Conv2d) and m != self.final_conv:
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, xt, x0, t, style_id):
        """
        Args:
            xt: [B, 4, H, W] - å½“å‰æµå½¢çŠ¶æ€
            x0: [B, 4, H, W] - åŸå§‹å†…å®¹é”šç‚¹ (ç»“æ„æ¡ä»¶)
            t: [B] - æ—¶é—´æ­¥
            style_id: [B] - é£æ ¼ ID
        Returns:
            v: [B, 4, H, W] - é¢„æµ‹æµé€Ÿ
        """
        # 1. å‡†å¤‡æ¡ä»¶
        t_emb = self.time_mlp(t)                 # [B, dim]
        style_emb = self.style_embed(style_id)   # [B, dim]
        
        # èåˆæ—¶é—´ä¸é£æ ¼ (ç®€å•çš„ç›¸åŠ æˆ–æ‹¼æ¥å‡å¯ï¼Œè¿™é‡Œé€‰æ‹©ç›¸åŠ ä½œä¸ºå…¨å±€ Condition)
        global_cond = t_emb + style_emb
        
        # 2. æ‹¼æ¥è¾“å…¥å¹¶è¿›å…¥ç‰¹å¾ç©ºé—´
        x = torch.cat([xt, x0], dim=1)
        x = self.stem(x)
        
        # 3. é€šè¿‡å¾®åˆ†æµå—
        for block in self.blocks:
            x = block(x, global_cond)
            
        # 4. é¢„æµ‹é€Ÿåº¦åœº
        x = self.final_norm(x)
        v = self.final_conv(x)
        
        return v