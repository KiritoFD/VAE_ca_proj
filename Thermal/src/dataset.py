import logging
import random
from pathlib import Path
from typing import Dict, List, Optional
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset

logger = logging.getLogger(__name__)

def elastic_deform(x: torch.Tensor, alpha: float = 15.0, sigma: int = 3, seed: Optional[int] = None) -> torch.Tensor:
    """
    [7940HX Optimized] å¼¹æ€§å½¢å˜ç®—å­
    ä¿®å¤äº†å½’ä¸€åŒ–é€»è¾‘ï¼Œå¢åŠ äº†ç¡®å®šæ€§æ§åˆ¶ã€‚
    """
    if seed is not None:
        torch.manual_seed(seed)
    
    if x.ndim != 4:
        x = x.unsqueeze(0)

    b, _, h, w = x.shape
    device = x.device

    # 1. å™ªå£°åœºç”Ÿæˆ (ä¸­å¿ƒåŒ–åˆ†å¸ƒ)
    dx = torch.randn(b, 1, h, w, device=device)
    dy = torch.randn(b, 1, h, w, device=device)

    # 2. ç¼“å­˜å‹å¥½çš„å¹³æ»‘ (æ›¿ä»£é«˜æ–¯æ¨¡ç³Š)
    # 3æ¬¡ AvgPool è¿‘ä¼¼é«˜æ–¯ï¼Œåœ¨ CPU ä¸Šåˆ©ç”¨ AVX æŒ‡ä»¤é›†æå¿«
    for _ in range(3):
        dx = F.avg_pool2d(dx, kernel_size=5, stride=1, padding=2)
        dy = F.avg_pool2d(dy, kernel_size=5, stride=1, padding=2)

    flow = torch.cat([dx, dy], dim=1) * alpha

    # 3. ç½‘æ ¼æ„å»ºä¸å½’ä¸€åŒ–
    y_grid, x_grid = torch.meshgrid(
        torch.arange(h, device=device), torch.arange(w, device=device), indexing='ij'
    )
    # [B, H, W, 2]
    grid_norm = torch.stack([x_grid, y_grid], dim=-1).float().unsqueeze(0).repeat(b, 1, 1, 1)
    
    # åæ ‡å½’ä¸€åŒ–: [0, W-1] -> [-1, 1]
    grid_norm[..., 0] = 2.0 * grid_norm[..., 0] / (w - 1) - 1.0
    grid_norm[..., 1] = 2.0 * grid_norm[..., 1] / (h - 1) - 1.0

    # ä½ç§»å½’ä¸€åŒ–: åƒç´ è·ç¦» -> ç›¸å¯¹è·ç¦»
    # âœ… ä¿®å¤æ ¸å¿ƒ Bug: ç›¸å¯¹ä½ç§»ä¸éœ€è¦å‡ 1ï¼Œå› ä¸ºå®ƒæœ¬èº«å°±æ˜¯ delta
    flow_norm = flow.permute(0, 2, 3, 1)
    flow_norm[..., 0] = 2.0 * flow_norm[..., 0] / (w - 1)
    flow_norm[..., 1] = 2.0 * flow_norm[..., 1] / (h - 1)

    # 4. é‡‡æ ·
    return F.grid_sample(x, grid_norm + flow_norm, mode='bilinear', padding_mode='reflection', align_corners=True)


class LatentDataset(Dataset):
    """
    [SWD-laplas] å‡è¡¡åŒ–æ•°æ®é›†
    ç‰¹æ€§ï¼šå†…å­˜é©»ç•™ã€ç¡®å®šæ€§é‡‡æ ·ã€åŠ¨æ€å¢å¼ºè°ƒåº¦
    """
    def __init__(
        self, 
        data_root: str, 
        num_styles: int, 
        style_subdirs: Optional[List[str]] = None, 
        config: dict = None
    ):
        self.data_root = Path(data_root)
        self.num_styles = num_styles
        self.style_subdirs = style_subdirs or [f"style{i}" for i in range(num_styles)]
        
        # é…ç½®è¯»å–
        train_cfg = config['training'] if config else {}
        self.apply_elastic = train_cfg.get('use_elastic_deform', True)
        self.base_alpha = train_cfg.get('elastic_alpha', 15.0)
        self.current_alpha = self.base_alpha
        self.current_epoch = 0

        # æ•°æ®åŠ è½½
        self.style_indices: Dict[int, List[int]] = {}
        latents_list = []
        current_idx = 0

        logger.info(f"Loading {self.num_styles} styles from {self.data_root}...")
        for style_id, subdir in enumerate(self.style_subdirs):
            files = sorted((self.data_root / subdir).glob("*.pt"))
            self.style_indices[style_id] = list(range(current_idx, current_idx + len(files)))
            for f in files:
                # ç§»é™¤ squeezeï¼Œä¿æŒ [C, H, W] ç»Ÿä¸€å¤„ç†
                latents_list.append(torch.load(f, map_location='cpu').float().squeeze(0))
                current_idx += 1
        
        if not latents_list:
            raise RuntimeError(f"No data found in {self.data_root}")

        self.latents_tensor = torch.stack(latents_list)
        
        # è‡ªåŠ¨ç¼©æ”¾æ£€æµ‹
        if self.latents_tensor.std() < 0.5:
            logger.info("Auto-scaling VAE latents by 1/0.18215")
            self.latents_tensor = self.latents_tensor / 0.18215
            
        # ğŸš€ å†…å­˜ä¼˜åŒ–ï¼šPin Memory åŠ é€Ÿå‘ 4070 ä¼ è¾“
        self.latents_tensor = self.latents_tensor.pin_memory()
        
        # è™šæ‹Ÿé•¿åº¦ï¼šä¿è¯æ¯ä¸ªé£æ ¼éƒ½å……åˆ†è¦†ç›–
        max_len = max(len(x) for x in self.style_indices.values())
        self.virtual_len = max_len * num_styles * num_styles * 2

    def set_epoch(self, epoch: int):
        """å¤–éƒ¨è°ƒç”¨ï¼šæ›´æ–°å¢å¼ºå¼ºåº¦"""
        self.current_epoch = epoch
        # åŠ¨æ€è°ƒåº¦ï¼š100 epoch åå¼€å§‹å¢å¼ºåŠ›åº¦
        scale = 1.0 + 0.3 * (max(0, epoch - 100) // 50)
        self.current_alpha = self.base_alpha * scale

    def __len__(self):
        return self.virtual_len

    def __getitem__(self, index):
        # âœ… ç¡®å®šæ€§é‡‡æ ·é€»è¾‘
        style_id = index % self.num_styles
        indices = self.style_indices[style_id]
        # ä¼ªéšæœºä½†ç¡®å®šæ€§çš„å†…éƒ¨ç´¢å¼•
        intra_idx = (index * 31337 + self.current_epoch) % len(indices)
        real_idx = indices[intra_idx]

        latent = self.latents_tensor[real_idx]
        
        # å¼‚æ­¥å¢å¼º (CPU Worker æ‰§è¡Œ)
        if self.apply_elastic:
            # ä¼ å…¥ index ä½œä¸ºç§å­ï¼Œä¿è¯åŒä¸€ä¸ªæ ·æœ¬åœ¨åŒä¸€ä¸ª epoch å¢å¼ºç»“æœä¸€è‡´
            latent_deformed = elastic_deform(
                latent.unsqueeze(0), 
                alpha=self.current_alpha, 
                seed=index + self.current_epoch * 10000
            ).squeeze(0)
        else:
            latent_deformed = latent

        return {
            'latent': latent,
            'latent_deformed': latent_deformed,
            'style_id': torch.tensor(style_id, dtype=torch.long)
        }

    def sample_style_batch(self, target_style_ids: torch.Tensor, device: torch.device) -> torch.Tensor:
        """SWD Loss ä¸“ç”¨çš„å¿«é€Ÿé‡‡æ ·"""
        b = target_style_ids.shape[0]
        out = torch.empty((b, *self.latents_tensor.shape[1:]), device=device)
        target_cpu = target_style_ids.cpu()

        for style_id, indices in self.style_indices.items():
            mask = (target_cpu == style_id)
            if not mask.any(): continue
            
            # è¿™é‡Œçš„éšæœºæ€§æ˜¯ä¸ºäº† SWD ç»Ÿè®¡ç‰¹æ€§ï¼Œä¿ç•™ random
            rand_idxs = torch.tensor(indices)[torch.randint(len(indices), (int(mask.sum()),))]
            out[mask] = self.latents_tensor[rand_idxs].to(device, non_blocking=True)
            
        return out