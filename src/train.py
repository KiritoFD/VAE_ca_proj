import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm
from pathlib import Path
import json
import random
import os
import sys
import logging
import time
import numpy as np
from PIL import Image
from diffusers import AutoencoderKL
import torch.nn.functional as F
import gc  # ğŸŸ¢ æ–°å¢ï¼šåƒåœ¾å›æ”¶

from SAFlow import SAFModel
from dataset import Stage1Dataset, Stage2Dataset

# -----------------------------------------------------------------------------
# ç¡¬ä»¶åŠ é€Ÿä¸å…¨å±€è®¾ç½®
# -----------------------------------------------------------------------------
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# ================= è¶…å‚æ•° =================
EVAL_STEP = 1           # æ¯å¤šå°‘ä¸ª Epoch è¿›è¡Œä¸€æ¬¡æ¨ç†éªŒè¯
MAX_GRAD_NORM = 1.0     # æ¢¯åº¦è£å‰ªé˜ˆå€¼
IDENTITY_PROB = 0.15    # Stage 1 è®­ç»ƒæ’ç­‰æ˜ å°„çš„æ¦‚ç‡
# =========================================

# ğŸŸ¢ æ–°å¢ï¼šé€’å½’æ›´æ–°å­—å…¸
def deep_update(source, overrides):
    """
    é€’å½’æ›´æ–°å­—å…¸ï¼Œç¡®ä¿å­å±‚çº§çš„é…ç½®ä¸ä¼šè¢«æš´åŠ›è¦†ç›–ï¼Œè€Œæ˜¯æŒ‰é”®æ›´æ–°ã€‚
    """
    for key, value in overrides.items():
        if isinstance(value, dict) and value:
            returned = deep_update(source.get(key, {}), value)
            source[key] = returned
        else:
            source[key] = overrides[key]
    return source

# ğŸŸ¢ [å‘½å] é¢‘è°±å¹…åº¦æŸå¤± (Spectral Amplitude Distance)
def compute_spectral_loss(v_pred, v_gt):
    """
    è®¡ç®—é¢‘åŸŸæŸå¤± (Spectral Amplitude Loss)ï¼Œå¼ºåˆ¶çº¹ç†ç»†èŠ‚å¯¹é½ã€‚
    åŸºäºå¸•å¡ç“¦å°”å®šç†ï¼Œåœ¨é¢‘åŸŸè®¡ç®— MSE ä»¥è§£å†³ç©ºåŸŸ MSE å¯¹é«˜é¢‘ä¸æ•æ„Ÿçš„é—®é¢˜ã€‚
    """
    # 1. å¼ºåˆ¶è½¬ float32 é¿å… FFT ç²¾åº¦æº¢å‡º
    v_pred = v_pred.float()
    v_gt = v_gt.float()

    # 2. FFT å˜æ¢ (Real-to-Complex)
    # norm='ortho' ä¿è¯èƒ½é‡å®ˆæ’
    fft_pred = torch.fft.rfft2(v_pred, norm='ortho')
    fft_gt = torch.fft.rfft2(v_gt, norm='ortho')
    
    # 3. è®¡ç®—å¹…åº¦è°± (Amplitude Spectrum)
    amp_pred = torch.abs(fft_pred)
    amp_gt = torch.abs(fft_gt)
    
    # 4. é¢‘åŸŸ MSE
    return F.mse_loss(amp_pred, amp_gt)

class TrainingLogger:
    def __init__(self, log_dir):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        # ğŸŸ¢ ä¿®æ”¹ï¼šä½¿ç”¨å”¯ä¸€çš„ logger åç§°é¿å…é‡åå†²çª
        self.logger = logging.getLogger(f"LSFM_{id(self)}")
        self.logger.setLevel(logging.INFO)
        self.logger.propagate = False  # ğŸŸ¢ æ–°å¢ï¼šé˜²æ­¢é‡å¤æ‰“å°
        if not self.logger.handlers:
            fh = logging.FileHandler(self.log_dir / "train.log", encoding='utf-8')
            ch = logging.StreamHandler(sys.stdout)
            fmt = logging.Formatter('[%(asctime)s][%(levelname)s] %(message)s', datefmt='%H:%M:%S')
            fh.setFormatter(fmt)
            ch.setFormatter(fmt)
            self.logger.addHandler(fh)
            self.logger.addHandler(ch)

    def info(self, msg): 
        self.logger.info(msg)

class LSFMTrainer:
    # ğŸŸ¢ æ ¸å¿ƒä¿®æ”¹ï¼šæ¥æ”¶ config_override å‚æ•°
    def __init__(self, config_override=None):
        # 1. åŠ è½½åŸºç¡€é…ç½®
        config_path = Path("config.json")
        if not config_path.exists(): config_path = Path("../config.json")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.cfg = json.load(f)
        
        # 2. ğŸŸ¢ æ·±åº¦åº”ç”¨é…ç½®è¦†ç›– (æ”¯æŒä»»æ„å‚æ•°ä¿®æ”¹)
        if config_override:
            self.cfg = deep_update(self.cfg, config_override)
        
        self.device = torch.device("cuda")
        self.ckpt_dir = Path(self.cfg['checkpoint']['save_dir'])
        self.ckpt_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = TrainingLogger(self.ckpt_dir / "logs")
        
        # ğŸŸ¢ æ–°å¢ï¼šä¿å­˜æœ¬æ¬¡å®éªŒçš„å®Œæ•´é…ç½®ï¼Œæ–¹ä¾¿å›æº¯
        with open(self.ckpt_dir / "experiment_config.json", "w", encoding='utf-8') as f:
            json.dump(self.cfg, f, indent=4, ensure_ascii=False)
        
        self.reflow_dir = Path(self.cfg['training'].get('reflow_data_dir', 'data/reflow_pairs'))
        
        # ğŸŸ¢ è¯»å–å¯é…ç½®çš„æŸå¤±æƒé‡
        self.transfer_weight = self.cfg['training'].get('transfer_loss_weight', 1.0)

        # ğŸŸ¢ è¯»å–æ ‡ç­¾ä¸¢å¼ƒæ¦‚ç‡ï¼ˆCFG è®­ç»ƒå¿…éœ€ï¼‰
        self.label_drop_prob = self.cfg['training'].get('label_drop_prob', 0.15)
        self.null_class_id = self.cfg['model']['num_styles']  # N+1 æ˜¯ç©ºç±»åˆ«
        
        self.logger.info("="*50)
        self.logger.info(f"ğŸš€ Initializing Experiment")
        self.logger.info(f"ğŸ“‚ Save Dir  : {self.ckpt_dir}")
        self.logger.info(f"Device      : {self.device}")
        self.logger.info(f"Batch Size  : {self.cfg['training']['batch_size']}")
        self.logger.info(f"Resolution  : 256x256")
        self.logger.info(f"âš¡ LR: {self.cfg['training']['learning_rate']} | Weight: {self.transfer_weight}")
        self.logger.info(f"ğŸ² Label Drop Prob: {self.label_drop_prob} (Null ID: {self.null_class_id})")
        self.logger.info("="*50)

        # 2. åŠ è½½ VAE (FT-MSE)
        self.logger.info("[Init] Loading VAE (stabilityai/sd-vae-ft-mse)...")
        self.vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse").to(self.device)
        self.vae.eval().requires_grad_(False).float()

        # 3. ç§»é™¤ LPIPS ä»¥èŠ‚çœæ˜¾å­˜ (æˆ‘ä»¬ç°åœ¨ç”¨ Spectral Loss æ›¿ä»£å®ƒ)
        # self.lpips = ... (Deleted)

        # 4. åˆå§‹åŒ–è®­ç»ƒæ•°æ®é›†
        self.logger.info("[Init] Loading Training Dataset...")
        self.train_ds = Stage1Dataset(self.cfg['data']['data_root'], self.cfg['data']['num_classes'])
        self.logger.info(f"[Init] Dataset Ready. Total Latents: {len(self.train_ds)}")

        # 5. æ¨ç†é¢„å¤„ç†
        self.infer_transform = transforms.Compose([
            transforms.Resize((256, 256)), 
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])

    def get_model(self):
        model = SAFModel(**self.cfg['model']).to(self.device, memory_format=torch.channels_last)
        self.logger.info("[Model] Compiling network with torch.compile (max-autotune)...")
        try:
            model = torch.compile(model, mode="max-autotune")
        except Exception as e:
            self.logger.info(f"[Model] Compile warning: {e}")
        return model

    def safe_load(self, model, state_dict, strict=True):
        clean_dict = self.clean_sd(state_dict)
        if hasattr(model, '_orig_mod'):
            model._orig_mod.load_state_dict(clean_dict, strict=strict)
        else:
            model.load_state_dict(clean_dict, strict=strict)

    def clean_sd(self, sd):
        if list(sd.keys())[0].startswith("_orig_mod."):
            return {k.replace("_orig_mod.", ""): v for k, v in sd.items()}
        return sd

    # -----------------------------------------------------------------------------
    # ğŸŸ¢ æ–°å¢ï¼šæ£€æŸ¥ç‚¹æ¢å¤é€»è¾‘
    # -----------------------------------------------------------------------------
    def find_latest_checkpoint(self, stage="stage1"):
        """æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒæ£€æŸ¥ç‚¹"""
        ckpt_pattern = f"{stage}_epoch*.pt"
        ckpts = list(self.ckpt_dir.glob(ckpt_pattern))
        if not ckpts:
            return None, 0
        
        # æå– epoch æ•°å­—å¹¶æ’åº
        import re
        ckpt_epochs = []
        for ckpt in ckpts:
            match = re.search(r'epoch(\d+)', ckpt.name)
            if match:
                ckpt_epochs.append((int(match.group(1)), ckpt))
        
        if not ckpt_epochs:
            return None, 0
        
        # è¿”å›æœ€æ–°çš„æ£€æŸ¥ç‚¹
        latest_epoch, latest_path = max(ckpt_epochs, key=lambda x: x[0])
        return latest_path, latest_epoch

    def load_training_state(self, checkpoint_path, model, optimizer, scheduler):
        """åŠ è½½å®Œæ•´çš„è®­ç»ƒçŠ¶æ€"""
        self.logger.info(f"ğŸ“‚ Loading checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # ğŸŸ¢ æ–°å¢ï¼šå‘åå…¼å®¹æ—§æ ¼å¼æ£€æŸ¥ç‚¹
        # æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯ state_dict (æ²¡æœ‰åŒ…è£¹åœ¨å­—å…¸ä¸­)
        # æ–°æ ¼å¼ï¼š{'model_state_dict': ..., 'optimizer_state_dict': ..., ...}
        if 'model_state_dict' not in checkpoint:
            # è¿™æ˜¯æ—§æ ¼å¼ï¼Œç›´æ¥å½“ä½œ state_dict åŠ è½½
            self.logger.info("âš ï¸  Old checkpoint format detected (no training state)")
            self.safe_load(model, checkpoint)
            return 0, float('inf')  # ä»å¤´å¼€å§‹è®­ç»ƒ
        
        # åŠ è½½æ¨¡å‹æƒé‡
        self.safe_load(model, checkpoint['model_state_dict'])
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if 'optimizer_state_dict' in checkpoint:
            try:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                self.logger.info("âœ… Optimizer state restored")
            except Exception as e:
                self.logger.info(f"âš ï¸  Failed to load optimizer state: {e}")
        
        # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
        if 'scheduler_state_dict' in checkpoint:
            try:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                self.logger.info("âœ… Scheduler state restored")
            except Exception as e:
                self.logger.info(f"âš ï¸  Failed to load scheduler state: {e}")
        
        start_epoch = checkpoint.get('epoch', 0)
        best_loss = checkpoint.get('best_loss', float('inf'))
        
        self.logger.info(f"âœ… Resumed from epoch {start_epoch}, best_loss={best_loss:.6f}")
        return start_epoch, best_loss

    # -----------------------------------------------------------------------------
    # Stage 1: Latent Structure Flow Matching
    # -----------------------------------------------------------------------------
    def construct_target_lsfm(self, x_c, x_s):
        x_c, x_s = x_c.float(), x_s.float()
        B, C, H, W = x_c.size()
        eps = 1e-5
        
        zc = x_c.view(B, C, -1)
        zs = x_s.view(B, C, -1)
        mu_c, mu_s = zc.mean(2, keepdim=True), zs.mean(2, keepdim=True)
        zc, zs = zc - mu_c, zs - mu_s
        
        cc = torch.bmm(zc, zc.transpose(1, 2)) / (H*W-1)
        cs = torch.bmm(zs, zs.transpose(1, 2)) / (H*W-1)
        
        try:
            uc, sc, _ = torch.linalg.svd(cc)
            us, ss, _ = torch.linalg.svd(cs)
        except: return x_c 
        
        c_inv = uc @ torch.diag_embed(1.0/torch.sqrt(sc.clamp(min=eps))) @ uc.transpose(1,2)
        s_mat = us @ torch.diag_embed(torch.sqrt(ss.clamp(min=eps))) @ us.transpose(1,2)
        z_wct = (s_mat @ c_inv @ zc + mu_s).view(B, C, H, W)
        
        fc = torch.fft.rfft2(z_wct, norm='ortho')
        fs = torch.fft.rfft2(x_s, norm='ortho')
        target = torch.fft.irfft2(torch.abs(fs) * torch.exp(1j * torch.angle(fc)), s=(H, W), norm='ortho')
        return target

    def run_stage1(self):
        # ğŸŸ¢ ä¿®æ”¹ï¼šæ£€æŸ¥ final è€Œéè·³è¿‡è®­ç»ƒ
        final_ckpt = self.ckpt_dir / "stage1_final.pt"
        
        self.logger.info("="*50)
        self.logger.info(">>> Starting Stage 1: LSFM Training")
        self.logger.info("="*50)
        
        model = self.get_model()
        dl = DataLoader(self.train_ds, batch_size=self.cfg['training']['batch_size'], 
                        shuffle=True, num_workers=8, pin_memory=True, drop_last=True)
        
        opt = torch.optim.AdamW(model.parameters(), lr=self.cfg['training']['learning_rate'])
        
        total_epochs = self.cfg['training']['stage1_epochs']
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            opt, 
            T_max=total_epochs, 
            eta_min=1e-6
        )

        # ğŸŸ¢ æ–°å¢ï¼šæ–­ç‚¹ç»­è®­é€»è¾‘
        start_epoch = 0
        best_loss = float('inf')
        
        # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦å·²å®Œæˆè®­ç»ƒ
        if final_ckpt.exists():
            self.logger.info("[Stage 1] âœ… Final checkpoint exists. Skipping training.")
            return
        
        # æŸ¥æ‰¾æœ€æ–°çš„ä¸­é—´æ£€æŸ¥ç‚¹
        latest_ckpt, latest_epoch = self.find_latest_checkpoint("stage1")
        if latest_ckpt:
            start_epoch, best_loss = self.load_training_state(
                latest_ckpt, model, opt, scheduler
            )

        # ğŸŸ¢ ä¿®æ”¹ï¼šä» start_epoch + 1 å¼€å§‹è®­ç»ƒ
        for epoch in range(start_epoch + 1, total_epochs + 1):
            model.train()
            epoch_loss = 0.0
            start_time = time.time()
            
            pbar = tqdm(dl, desc=f"[S1] Epoch {epoch}/{total_epochs}", leave=False)
            
            for x_c, x_s, t_id, s_id in pbar:
                x_c = x_c.to(self.device, memory_format=torch.channels_last, non_blocking=True)
                x_s = x_s.to(self.device, memory_format=torch.channels_last, non_blocking=True)
                t_id, s_id = t_id.to(self.device, non_blocking=True), s_id.to(self.device, non_blocking=True)

                with torch.no_grad():
                    # ğŸŸ¢ Identity é‡‡æ ·
                    if random.random() < IDENTITY_PROB:
                        x_s, t_id = x_c.clone(), s_id.clone()
                    
                    # ğŸŸ¢ å…³é”®ï¼šLabel Dropping (CFG è®­ç»ƒ)
                    # ä»¥ä¸€å®šæ¦‚ç‡å°† t_id æ›¿æ¢ä¸º null_class_id
                    drop_mask = torch.rand(t_id.shape[0], device=self.device) < self.label_drop_prob
                    t_id_dropped = torch.where(drop_mask, 
                                               torch.full_like(t_id, self.null_class_id), 
                                               t_id)
                    
                    target = self.construct_target_lsfm(x_c, x_s).to(memory_format=torch.channels_last)
                    is_id = (s_id == t_id).view(-1, 1, 1, 1).float()
                    target = is_id * x_c + (1 - is_id) * target

                opt.zero_grad(set_to_none=True)
                with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16):
                    v_gt = target - x_c
                    
                    t = torch.rand(x_c.size(0), device=self.device)
                    x_t = (1 - t.view(-1,1,1,1)) * x_c + t.view(-1,1,1,1) * target
                    
                    # ğŸŸ¢ ä½¿ç”¨ dropped çš„ label è®­ç»ƒ
                    v_pred = model(x_t, x_c, t, t_id_dropped)
                    
                    # ğŸŸ¢ åŸºç¡€ MSE
                    loss_mse_raw = F.mse_loss(v_pred, v_gt, reduction='none')
                    loss_mse_per_sample = loss_mse_raw.mean(dim=[1, 2, 3])
                    
                    # ğŸŸ¢ è¾…åŠ©æŸå¤±ä»…ç”¨äºéç©ºç±»åˆ«çš„è½¬æ¢ä»»åŠ¡
                    is_transfer = (s_id != t_id).float()
                    is_not_null = (t_id_dropped != self.null_class_id).float()
                    apply_aux_loss = (is_transfer * is_not_null).bool()
                    
                    if apply_aux_loss.any():
                        loss_spec = compute_spectral_loss(
                            v_pred[apply_aux_loss], 
                            v_gt[apply_aux_loss]
                        )
                        
                        v_pred_flat = v_pred[apply_aux_loss].flatten(1)
                        v_gt_flat = v_gt[apply_aux_loss].flatten(1)
                        cos_sim = F.cosine_similarity(v_pred_flat, v_gt_flat, dim=1, eps=1e-6)
                        loss_dir = (1 - cos_sim).mean()
                    else:
                        loss_spec = torch.tensor(0.0, device=self.device)
                        loss_dir = torch.tensor(0.0, device=self.device)
                    
                    # ğŸŸ¢ åŠ æƒ MSE
                    sample_weights = 1.0 + is_transfer * (self.transfer_weight - 1.0)
                    weighted_mse = (loss_mse_per_sample * sample_weights).mean()
                    
                    # ğŸŸ¢ æ€»æŸå¤±
                    loss = weighted_mse + 0.1 * loss_spec + 0.1 * loss_dir

                loss.backward()
                nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)
                opt.step()
                
                epoch_loss += loss.item()
                pbar.set_postfix(loss=f"{loss.item():.4f}")

            avg_loss = epoch_loss / len(dl)
            elapsed = time.time() - start_time
            
            # ğŸŸ¢ æ›´æ–° best_loss
            if avg_loss < best_loss:
                best_loss = avg_loss
            
            current_lr = scheduler.get_last_lr()[0]
            self.logger.info(f"[S1] Epoch {epoch:03d} | Avg Loss: {avg_loss:.6f} | Best: {best_loss:.6f} | LR: {current_lr:.2e} | Time: {elapsed:.1f}s")
            
            scheduler.step()

            # ğŸŸ¢ ä¿®æ”¹ï¼šä¿å­˜åŒ…å«è®­ç»ƒçŠ¶æ€çš„æ£€æŸ¥ç‚¹
            if epoch % EVAL_STEP == 0:
                self.save_ckpt(model, opt, scheduler, epoch, avg_loss, best_loss, "stage1")
                self.do_inference(model, epoch, "stage1")
        
        # ğŸŸ¢ æœ€åä¿å­˜ final æ£€æŸ¥ç‚¹
        self.logger.info("[Stage 1] Training Completed. Saving final checkpoint...")
        torch.save({
            'epoch': total_epochs,
            'model_state_dict': self.clean_sd(model.state_dict()),
            'optimizer_state_dict': opt.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_loss': best_loss,
            'config': self.cfg
        }, final_ckpt)

    # -----------------------------------------------------------------------------
    # Intermediate: Reflow Data Generation
    # -----------------------------------------------------------------------------
    @torch.no_grad()
    def generate_reflow_data(self):
        if self.reflow_dir.exists() and len(list(self.reflow_dir.glob("*.pt"))) > 0:
            return

        self.logger.info("="*50)
        self.logger.info(">>> Generating Reflow Data (Latent ODE Sampling)")
        self.logger.info("="*50)
        
        model = self.get_model()
        
        # ğŸŸ¢ ä¿®å¤ï¼šåŠ è½½æ—¶å¤„ç†æ–°æ—§æ ¼å¼
        stage1_ckpt = torch.load(self.ckpt_dir / "stage1_final.pt", map_location=self.device)
        if 'model_state_dict' in stage1_ckpt:
            self.safe_load(model, stage1_ckpt['model_state_dict'])
        else:
            self.safe_load(model, stage1_ckpt)
        
        model.eval()
        
        self.reflow_dir.mkdir(parents=True, exist_ok=True)
        dl = DataLoader(self.train_ds, batch_size=self.cfg['training']['batch_size'], 
                        shuffle=False, num_workers=8, pin_memory=True)
        
        steps, dt = 10, 0.1
        count = 0
        
        for x_c, _, _, _ in tqdm(dl, desc="Generating Pairs"):
            x_c = x_c.to(self.device, memory_format=torch.channels_last, non_blocking=True)
            for tid in range(self.cfg['data']['num_classes']):
                t_vec = torch.full((x_c.size(0),), tid, device=self.device, dtype=torch.long)
                x_t = x_c.clone()
                for i in range(steps):
                    t = torch.ones(x_c.size(0), device=self.device) * (i * dt)
                    with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16):
                        v = model(x_t, x_c, t, t_vec)
                    x_t = x_t + v.float() * dt
                
                torch.save({'z0': x_c.cpu(), 'z1': x_t.cpu(), 't_id': t_vec.cpu()}, 
                           self.reflow_dir / f"b{count}_c{tid}.pt")
            count += 1
            
        self.logger.info(f"[Reflow] Generation Finished. Saved to {self.reflow_dir}")

    # -----------------------------------------------------------------------------
    # Stage 2: Reflow (Distillation)
    # -----------------------------------------------------------------------------
    def run_stage2(self):
        # ğŸŸ¢ åŒæ ·çš„æ–­ç‚¹ç»­è®­é€»è¾‘
        final_ckpt = self.ckpt_dir / "stage2_final.pt"
        
        self.logger.info("="*50)
        self.logger.info(">>> Starting Stage 2: Distillation (Reflow)")
        self.logger.info("="*50)
        
        model = self.get_model()
        
        # ğŸŸ¢ ä¿®å¤ï¼šåŠ è½½ stage1_final æ—¶å¤„ç†æ–°æ—§æ ¼å¼
        stage1_ckpt = torch.load(self.ckpt_dir / "stage1_final.pt", map_location=self.device)
        if 'model_state_dict' in stage1_ckpt:
            self.safe_load(model, stage1_ckpt['model_state_dict'], strict=False)
        else:
            self.safe_load(model, stage1_ckpt, strict=False)
        
        ds = Stage2Dataset(self.reflow_dir)
        dl = DataLoader(ds, batch_size=self.cfg['training']['batch_size'], 
                        shuffle=True, num_workers=8, pin_memory=True, 
                        persistent_workers=True, drop_last=True)
        
        opt = torch.optim.AdamW(model.parameters(), lr=self.cfg['training']['learning_rate'])
        total_epochs = self.cfg['training']['stage2_epochs']
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            opt, 
            T_max=total_epochs, 
            eta_min=1e-6
        )
        
        # ğŸŸ¢ æ–­ç‚¹ç»­è®­
        start_epoch = 0
        best_loss = float('inf')
        
        if final_ckpt.exists():
            self.logger.info("[Stage 2] âœ… Final checkpoint exists. Skipping training.")
            return
        
        latest_ckpt, latest_epoch = self.find_latest_checkpoint("stage2")
        if latest_ckpt:
            start_epoch, best_loss = self.load_training_state(
                latest_ckpt, model, opt, scheduler
            )
        
        for epoch in range(start_epoch + 1, total_epochs + 1):
            model.train()
            epoch_loss = 0.0
            start_time = time.time()
            
            pbar = tqdm(dl, desc=f"[S2] Epoch {epoch}/{total_epochs}", leave=False)
            
            for z0, z1, t_id in pbar:
                z0 = z0.to(self.device, memory_format=torch.channels_last, non_blocking=True)
                z1 = z1.to(self.device, memory_format=torch.channels_last, non_blocking=True)
                t_id = t_id.to(self.device, non_blocking=True)
                
                opt.zero_grad(set_to_none=True)
                with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16):
                    t = torch.rand(z0.size(0), device=self.device)
                    x_t = (1-t.view(-1,1,1,1)) * z0 + t.view(-1,1,1,1) * z1
                    v_pred = model(x_t, z0, t, t_id)
                    loss = F.mse_loss(v_pred, z1 - z0)
                
                loss.backward()
                opt.step()
                
                epoch_loss += loss.item()
                pbar.set_postfix(loss=f"{loss.item():.4f}")
            
            avg_loss = epoch_loss / len(dl)
            elapsed = time.time() - start_time
            
            if avg_loss < best_loss:
                best_loss = avg_loss
            
            current_lr = scheduler.get_last_lr()[0]
            self.logger.info(f"[S2] Epoch {epoch:03d} | Avg Loss: {avg_loss:.6f} | Best: {best_loss:.6f} | LR: {current_lr:.2e} | Time: {elapsed:.1f}s")
            
            scheduler.step()
            
            if epoch % EVAL_STEP == 0:
                self.save_ckpt(model, opt, scheduler, epoch, avg_loss, best_loss, "stage2")
                self.do_inference(model, epoch, "stage2", steps_override=4)
        
        # ä¿å­˜ final
        torch.save({
            'epoch': total_epochs,
            'model_state_dict': self.clean_sd(model.state_dict()),
            'optimizer_state_dict': opt.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_loss': best_loss,
            'config': self.cfg
        }, final_ckpt)

    # -----------------------------------------------------------------------------
    # Inference / Validation
    # -----------------------------------------------------------------------------

    @torch.no_grad()
    def do_inference(self, model, epoch, stage, steps_override=None):
        model.eval()
        
        with open("config.json", 'r', encoding='utf-8') as f:
            fresh_cfg = json.load(f)
        
        inf_cfg = self.cfg.get('inference', {})
        # ğŸŸ¢ å¼ºè¡Œæ¸…æ´—å­—ç¬¦ä¸²ï¼šç¡®ä¿ monet2photo ä¸­åªæœ‰ä¸€ä¸ª p
        raw_path = inf_cfg.get('image_path', '').replace("monet2pphoto", "monet2photo")
        test_root = Path(raw_path)
        
        if not test_root.exists():
            self.logger.info(f"[Inference] âŒ Path not found: {test_root}")
            model.train()
            return
        
        # ğŸŸ¢ [ä¿®æ”¹] æ¨ç†ç»“æœä¿å­˜åˆ° checkpoint ç›®å½•ä¸‹çš„ inf å­ç›®å½•
        save_root = self.ckpt_dir / "inf" / stage / f"ep{epoch}"
        save_root.mkdir(parents=True, exist_ok=True)
        
        steps = steps_override if steps_override else inf_cfg.get('num_inference_steps', 5)
        cfg_scale = inf_cfg.get('cfg_scale', 2.0)
        use_cfg = inf_cfg.get('use_cfg', True)
        latent_clamp = inf_cfg.get('latent_clamp', 3.0)
        
        dt = 1.0 / steps

        # éå†å­ç›®å½•
        subdirs = [d for d in test_root.iterdir() if d.is_dir()]
        if not subdirs: subdirs = [test_root]

        self.logger.info(f"[Inference] Starting Epoch {epoch} | Steps: {steps}")

        for subdir in subdirs:
            files = []
            for ext in ['*.jpg', '*.JPG', '*.png', '*.PNG', '*.jpeg']:
                files.extend(list(subdir.glob(ext)))
            
            files = sorted(files)[:2] # æ¯ä¸ªå­ç±»åªè·‘2å¼ ï¼ŒèŠ‚çœæ—¶é—´
            if not files: continue

            for img_p in files:
                try:
                    img = Image.open(img_p).convert("RGB")
                    pixel = self.infer_transform(img).unsqueeze(0).to(self.device)
                    z_c = self.vae.encode(pixel).latent_dist.sample() * 0.18215
                    z_c = z_c.to(memory_format=torch.channels_last)
                    
                    self.save_img(pixel, save_root / f"{subdir.name}_{img_p.stem}_orig.jpg")
                    
                    for tid in range(self.cfg['data']['num_classes']):
                        z_t = z_c.clone()
                        t_vec = torch.tensor([tid], device=self.device)
                        
                        # ğŸŸ¢ CFG æ¨ç†
                        for k in range(steps):
                            t_step = torch.ones(1, device=self.device) * (k * dt)
                            
                            with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16):
                                if use_cfg:
                                    # æ¡ä»¶é¢„æµ‹
                                    v_cond = model(z_t, z_c, t_step, t_vec)
                                    
                                    # æ— æ¡ä»¶é¢„æµ‹
                                    null_vec = torch.tensor([self.null_class_id], device=self.device)
                                    v_uncond = model(z_t, z_c, t_step, null_vec)
                                    
                                    # ğŸŸ¢ CFG å…¬å¼: v = v_uncond + scale * (v_cond - v_uncond)
                                    v = v_uncond + cfg_scale * (v_cond - v_uncond)
                                else:
                                    v = model(z_t, z_c, t_step, t_vec)
                            
                            # æ¬§æ‹‰ç§¯åˆ†
                            z_t = z_t + v.float() * dt
                            
                            # ğŸŸ¢ æ•°å€¼è£å‰ªï¼ˆé˜²æ­¢çˆ†ç‚¸ï¼‰
                            if latent_clamp > 0:
                                z_t = z_t.clamp(-latent_clamp, latent_clamp)
                        
                        # ğŸŸ¢ å…³é”®ï¼šé™¤ä»¥ç¼©æ”¾å› å­ï¼Œä¿è¯è§£ç æ¸…æ™°åº¦
                        res_pixel = self.vae.decode(z_t.float() / 0.18215).sample
                        self.save_img(res_pixel, save_root / f"{subdir.name}_{img_p.stem}_to_S{tid}.jpg")
                    
                except Exception as e:
                    self.logger.info(f"[Inference] Error processing {img_p.name}: {e}")

        self.logger.info(f"[Inference] Successfully finished. Output: {save_root}")
        model.train()

    def save_ckpt(self, model, optimizer, scheduler, epoch, loss, best_loss, stage):
        """ä¿å­˜åŒ…å«å®Œæ•´è®­ç»ƒçŠ¶æ€çš„æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.clean_sd(model.state_dict()),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'loss': loss,
            'best_loss': best_loss,
            'config': self.cfg
        }
        
        path = self.ckpt_dir / f"{stage}_epoch{epoch}.pt"
        torch.save(checkpoint, path)
        
        # ğŸŸ¢ ä¿ç•™æœ€è¿‘ 3 ä¸ªæ£€æŸ¥ç‚¹
        ckpts = sorted(list(self.ckpt_dir.glob(f"{stage}_epoch*.pt")), key=os.path.getmtime)
        if len(ckpts) > 3: 
            for old_ckpt in ckpts[:-3]:
                os.remove(old_ckpt)
                self.logger.info(f"ğŸ—‘ï¸  Removed old checkpoint: {old_ckpt.name}")

    def save_img(self, tensor, path):
        img = (tensor.cpu().permute(0,2,3,1).numpy()[0] * 0.5 + 0.5).clip(0, 1)
        Image.fromarray((img * 255).astype('uint8')).save(path)

    def run_pipeline(self):
        self.run_stage1()
        self.generate_reflow_data()
        self.run_stage2()

if __name__ == "__main__":
    LSFMTrainer().run_pipeline()