"""
æ‰¹é‡è¯„ä¼°è„šæœ¬
è‡ªåŠ¨æ‰«æ AutoSearch_Results ç›®å½•ï¼Œå¯¹æ‰€æœ‰å·²å®Œæˆçš„å®éªŒè¿è¡Œè¯„ä¼°å¹¶æ±‡æ€»ç»“æœ
"""
import sys
import json
import torch
import gc
from pathlib import Path
import time
from tqdm import tqdm

# å¯¼å…¥è¯„ä¼°æ¨¡å—
from eval_lpips import Evaluator as LPIPSEvaluator
from eval_clip import Evaluator as CLIPEvaluator
from eval_vgg import Evaluator as VGGEvaluator


def find_all_experiments(root_dir="AutoSearch_Results"):
    """
    æ‰«ææ ¹ç›®å½•ï¼Œæ‰¾åˆ°æ‰€æœ‰åŒ…å« stage1_final.pt çš„å®éªŒ
    """
    root = Path(root_dir)
    if not root.exists():
        print(f"âŒ Directory not found: {root}")
        return []
    
    experiments = []
    for exp_dir in root.iterdir():
        if not exp_dir.is_dir():
            continue
        
        ckpt_dir = exp_dir / "checkpoints"
        final_ckpt = ckpt_dir / "stage1_final.pt"
        
        if final_ckpt.exists():
            experiments.append({
                "name": exp_dir.name,
                "exp_dir": exp_dir,
                "ckpt_dir": ckpt_dir,
                "checkpoint": final_ckpt
            })
    
    return experiments


def evaluate_single_experiment(exp_info, config_path="config.json"):
    """
    å¯¹å•ä¸ªå®éªŒè¿è¡Œæ‰€æœ‰è¯„ä¼°
    """
    exp_name = exp_info["name"]
    ckpt_path = exp_info["checkpoint"]
    ckpt_dir = exp_info["ckpt_dir"]
    
    # åˆ›å»ºè¯„ä¼°ç›®å½•
    eval_dir = ckpt_dir / "evaluation"
    eval_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Evaluating: {exp_name}")
    print(f"ğŸ“‚ Checkpoint: {ckpt_path}")
    print(f"ğŸ’¾ Results: {eval_dir}")
    print(f"{'='*60}")
    
    # è¯»å–é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        cfg = json.load(f)
    
    ref_dir = cfg.get("data", {}).get("data_root", "").strip('"').strip("'")
    target_dir = cfg.get("inference", {}).get("image_path", "").strip('"').strip("'")
    
    # æ±‡æ€»ç»“æœ
    results_summary = {
        "experiment_name": exp_name,
        "checkpoint_path": str(ckpt_path),
        "evaluation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
        "metrics": {}
    }
    
    # ========== 1. LPIPS Evaluation ==========
    try:
        print("\nğŸ”¹ [1/3] Running LPIPS Evaluation...")
        lpips_eval = LPIPSEvaluator(str(ckpt_path), config_path)
        
        # ğŸŸ¢ ä¿®å¤ï¼šç›´æ¥æ•è·è¯„ä¼°ç»“æœ
        lpips_eval.evaluate(target_dir, batch_size=2)
        
        # ğŸŸ¢ ä»è¯„ä¼°å™¨å†…éƒ¨å±æ€§æˆ–ç”Ÿæˆçš„æ–‡ä»¶è¯»å–ç»“æœ
        # æ–¹æ¡ˆ1: æ£€æŸ¥è¯„ä¼°å™¨æ˜¯å¦æœ‰resultså±æ€§
        if hasattr(lpips_eval, 'results'):
            lpips_data = lpips_eval.results
        else:
            # æ–¹æ¡ˆ2: è¯»å–é»˜è®¤è¾“å‡ºçš„CSVæ–‡ä»¶
            import pandas as pd
            csv_candidates = [
                Path("lpips_results.csv"),
                Path("eval_results/lpips_results.csv"),
                eval_dir / "lpips_results.csv"
            ]
            
            lpips_data = None
            for csv_path in csv_candidates:
                if csv_path.exists():
                    df = pd.read_csv(csv_path)
                    lpips_data = {
                        "mean": float(df['lpips'].mean()),
                        "std": float(df['lpips'].std()),
                        "per_style": {}
                    }
                    
                    # æŒ‰é£æ ¼ç»Ÿè®¡
                    if 'target_style' in df.columns:
                        for style in df['target_style'].unique():
                            style_df = df[df['target_style'] == style]
                            lpips_data["per_style"][f"style_{style}"] = {
                                "mean": float(style_df['lpips'].mean()),
                                "count": len(style_df)
                            }
                    
                    # ç§»åŠ¨åˆ°è¯„ä¼°ç›®å½•
                    target_path = eval_dir / "lpips_results.csv"
                    if csv_path != target_path:
                        csv_path.rename(target_path)
                    lpips_data["results_file"] = str(target_path)
                    break
            
            if lpips_data is None:
                lpips_data = {"error": "No output file found"}
        
        results_summary["metrics"]["lpips"] = lpips_data
        
        del lpips_eval
        gc.collect()
        torch.cuda.empty_cache()
        print("âœ… LPIPS Complete")
        
    except Exception as e:
        print(f"âŒ LPIPS Failed: {e}")
        import traceback
        traceback.print_exc()
        results_summary["metrics"]["lpips"] = {"error": str(e)}
    
    # ========== 2. CLIP Evaluation ==========
    try:
        print("\nğŸ”¹ [2/3] Running CLIP Evaluation...")
        clip_eval = CLIPEvaluator(str(ckpt_path), config_path)
        
        clip_eval.evaluate(target_dir, batch_size=2)
        
        import pandas as pd
        csv_candidates = [
            Path("clip_scores.csv"),
            Path("eval_results/clip_scores.csv"),
            eval_dir / "clip_scores.csv"
        ]
        
        clip_data = None
        for csv_path in csv_candidates:
            if csv_path.exists():
                df = pd.read_csv(csv_path)
                clip_data = {
                    "mean_similarity": float(df['clip_similarity'].mean()),
                    "std_similarity": float(df['clip_similarity'].std()),
                    "per_style": {}
                }
                
                if 'target_style' in df.columns:
                    for style in df['target_style'].unique():
                        style_df = df[df['target_style'] == style]
                        clip_data["per_style"][f"style_{style}"] = {
                            "mean": float(style_df['clip_similarity'].mean()),
                            "count": len(style_df)
                        }
                
                target_path = eval_dir / "clip_scores.csv"
                if csv_path != target_path:
                    csv_path.rename(target_path)
                clip_data["results_file"] = str(target_path)
                break
        
        if clip_data is None:
            clip_data = {"error": "No output file found or unrecognized format"}
        
        results_summary["metrics"]["clip"] = clip_data
        
        del clip_eval
        gc.collect()
        torch.cuda.empty_cache()
        print("âœ… CLIP Complete")
        
    except Exception as e:
        print(f"âŒ CLIP Failed: {e}")
        import traceback
        traceback.print_exc()
        results_summary["metrics"]["clip"] = {"error": str(e)}
    
    # ========== 3. VGG Style Evaluation ==========
    try:
        print("\nğŸ”¹ [3/3] Running VGG Style Evaluation...")
        vgg_eval = VGGEvaluator(str(ckpt_path), ref_root=ref_dir, config_path=config_path)
        
        vgg_eval.evaluate(bs=1)
        
        import pandas as pd
        csv_candidates = [
            Path("vgg_style_distances.csv"),
            Path("eval_results/vgg_style_distances.csv"),
            eval_dir / "vgg_style_distances.csv"
        ]
        
        vgg_data = None
        for csv_path in csv_candidates:
            if csv_path.exists():
                df = pd.read_csv(csv_path)
                
                # ğŸŸ¢ VGGçš„åˆ—åå¯èƒ½ä¸åŒï¼Œæ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„åˆ—
                distance_col = None
                for col in ['vgg_distance', 'style_distance', 'gram_distance']:
                    if col in df.columns:
                        distance_col = col
                        break
                
                if distance_col:
                    vgg_data = {
                        "mean_distance": float(df[distance_col].mean()),
                        "std_distance": float(df[distance_col].std()),
                        "per_style": {}
                    }
                    
                    if 'target_style' in df.columns:
                        for style in df['target_style'].unique():
                            style_df = df[df['target_style'] == style]
                            vgg_data["per_style"][f"style_{style}"] = {
                                "mean": float(style_df[distance_col].mean()),
                                "count": len(style_df)
                            }
                    
                    target_path = eval_dir / "vgg_style_distances.csv"
                    if csv_path != target_path:
                        csv_path.rename(target_path)
                    vgg_data["results_file"] = str(target_path)
                    break
        
        if vgg_data is None:
            vgg_data = {"error": "No output file found or unrecognized format"}
        
        results_summary["metrics"]["vgg"] = vgg_data
        
        del vgg_eval
        gc.collect()
        torch.cuda.empty_cache()
        print("âœ… VGG Complete")
        
    except Exception as e:
        print(f"âŒ VGG Failed: {e}")
        import traceback
        traceback.print_exc()
        results_summary["metrics"]["vgg"] = {"error": str(e)}
    
    # ä¿å­˜æ±‡æ€»
    summary_path = eval_dir / "metrics_summary.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, indent=4, ensure_ascii=False)
    
    print(f"\nâœ… Evaluation complete for {exp_name}")
    print(f"ğŸ“„ Summary: {summary_path}\n")
    
    return results_summary


def generate_comparison_table(all_results, output_path="AutoSearch_Results/comparison_table.csv"):
    """
    ç”Ÿæˆæ‰€æœ‰å®éªŒçš„å¯¹æ¯”è¡¨æ ¼
    """
    import pandas as pd
    
    rows = []
    for result in all_results:
        row = {
            "Experiment": result["experiment_name"],
            "Eval_Time": result["evaluation_time"]
        }
        
        # ğŸŸ¢ æ”¹è¿›ï¼šæ›´å¥å£®çš„æ•°æ®æå–
        # LPIPS
        lpips = result["metrics"].get("lpips", {})
        if "mean" in lpips:
            row["LPIPS_Mean"] = f"{lpips['mean']:.5f}"
            row["LPIPS_Std"] = f"{lpips['std']:.5f}"
        elif "error" in lpips:
            row["LPIPS_Mean"] = f"ERROR: {lpips['error']}"
            row["LPIPS_Std"] = "-"
        else:
            row["LPIPS_Mean"] = "NO DATA"
            row["LPIPS_Std"] = "-"
        
        # CLIP
        clip = result["metrics"].get("clip", {})
        if "mean_similarity" in clip:
            row["CLIP_Similarity"] = f"{clip['mean_similarity']:.5f}"
            row["CLIP_Std"] = f"{clip.get('std_similarity', 0):.5f}"
        elif "error" in clip:
            row["CLIP_Similarity"] = f"ERROR: {clip['error']}"
            row["CLIP_Std"] = "-"
        else:
            row["CLIP_Similarity"] = "NO DATA"
            row["CLIP_Std"] = "-"
        
        # VGG
        vgg = result["metrics"].get("vgg", {})
        if "mean_distance" in vgg:
            row["VGG_Distance"] = f"{vgg['mean_distance']:.5f}"
            row["VGG_Std"] = f"{vgg.get('std_distance', 0):.5f}"
        elif "error" in vgg:
            row["VGG_Distance"] = f"ERROR: {vgg['error']}"
            row["VGG_Std"] = "-"
        else:
            row["VGG_Distance"] = "NO DATA"
            row["VGG_Std"] = "-"
        
        rows.append(row)
    
    df = pd.DataFrame(rows)
    df.to_csv(output_path, index=False)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Comparison Table Generated")
    print(f"ğŸ“„ Saved to: {output_path}")
    print(f"{'='*60}\n")
    print(df.to_string(index=False))
    
    return df


def main():
    print("="*60)
    print("ğŸ” Batch Evaluation Script")
    print("="*60)
    
    # 1. æ‰«æå®éªŒ
    experiments = find_all_experiments()
    
    if not experiments:
        print("âŒ No experiments found with stage1_final.pt")
        return
    
    print(f"\nâœ… Found {len(experiments)} completed experiments:")
    for i, exp in enumerate(experiments, 1):
        print(f"  {i}. {exp['name']}")
    
    # 2. æ‰¹é‡è¯„ä¼°
    all_results = []
    for exp in tqdm(experiments, desc="Evaluating Experiments"):
        try:
            result = evaluate_single_experiment(exp)
            all_results.append(result)
        except Exception as e:
            print(f"\nâŒ Failed to evaluate {exp['name']}: {e}")
            import traceback
            traceback.print_exc()
    
    # 3. ç”Ÿæˆå¯¹æ¯”è¡¨
    if all_results:
        generate_comparison_table(all_results)
    
    print("\n" + "="*60)
    print("ğŸ‰ Batch Evaluation Complete!")
    print("="*60)


if __name__ == "__main__":
    main()
