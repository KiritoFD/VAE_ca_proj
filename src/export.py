import torch
import torch.nn as nn
import os
import json
import numpy as np
from PIL import Image
from diffusers import AutoencoderKL
from SAFlow import SAFModel 

# ================= é…ç½® =================
CKPT_PATH = r"g:\GitHub\VAE_ca_proj\checkpoints\stage1_epoch10.pt"
OUTPUT_DIR = "./mnn_export_final"
CONFIG_PATH = "config.json"
TEST_IMG_PATH = "test.jpg" 
# =======================================

def load_config():
    with open(CONFIG_PATH, 'r') as f: return json.load(f)

class EncoderWrapper(nn.Module):
    def __init__(self, vae):
        super().__init__()
        self.vae = vae
    def forward(self, x):
        return self.vae.encode(x).latent_dist.mode() * 0.18215

class FlowWrapper(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
    def forward(self, x_t, x_cond, t, s):
        return self.model(x_t, x_cond, t, s)

class DecoderWrapper(nn.Module):
    def __init__(self, vae):
        super().__init__()
        self.vae = vae
    def forward(self, z):
        z = z / 0.18215
        out = self.vae.decode(z).sample
        return (out / 2.0) + 0.5 # ç›´æ¥è¾“å‡º 0~1

def export():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    device = torch.device("cpu")
    
    cfg = load_config()
    saf_model = SAFModel(**cfg['model']).to(device)
    saf_model.load_state_dict(torch.load(CKPT_PATH, map_location=device))
    saf_model.eval()
    
    vae = AutoencoderKL.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="vae").to(device)
    vae.eval()
    
    # åŒ…è£…æ¨¡å‹
    encoder = EncoderWrapper(vae)
    flow_net = FlowWrapper(saf_model)
    decoder = DecoderWrapper(vae)
    
    # å‡†å¤‡ Dummy Inputs
    print("ğŸ“· æ­£åœ¨æ„é€  Dummy Inputs...")
    img = Image.open(TEST_IMG_PATH).convert("RGB").resize((512, 512))
    img_tensor = torch.from_numpy(np.array(img).astype(np.float32) / 127.5 - 1.0).permute(2, 0, 1).unsqueeze(0)
    
    with torch.no_grad():
        latent_c = encoder(img_tensor)
    
    # ã€å…³é”®ã€‘x_t å’Œ x_cond å¿…é¡»ä½¿ç”¨ç‰©ç†ä¸Šä¸åŒçš„ Tensor å®ä¾‹ï¼Œé˜²æ­¢è¢« ONNX ä¼˜åŒ–å™¨åˆå¹¶
    dummy_xt = torch.randn(1, 4, 64, 64)
    dummy_xc = latent_c.clone() 
    dummy_t = torch.tensor([0.5]).float()
    dummy_s = torch.tensor([0]).int()

    # Opset 14 + Classic Mode (Training=False å¼ºåˆ¶è§¦å‘æ—§ç‰ˆå¯¼å‡ºé€»è¾‘)
    common_args = {
        "opset_version": 14,
        "do_constant_folding": True,
        "keep_initializers_as_inputs": False,
        "training": torch.onnx.TrainingMode.EVAL
    }

    print(">>> å¯¼å‡º Encoder...")
    torch.onnx.export(encoder, img_tensor, f"{OUTPUT_DIR}/Encoder.onnx",
                      input_names=['input'], output_names=['output'], **common_args)
    
    print(">>> å¯¼å‡º Flow...")
    torch.onnx.export(flow_net, (dummy_xt, dummy_xc, dummy_t, dummy_s), f"{OUTPUT_DIR}/Flow.onnx",
                      input_names=['x_t', 'x_cond', 't', 's'], output_names=['output'], **common_args)
    
    print(">>> å¯¼å‡º Decoder...")
    torch.onnx.export(decoder, dummy_xc, f"{OUTPUT_DIR}/Decoder.onnx",
                      input_names=['input'], output_names=['output'], **common_args)
    
    print(f"âœ… å¯¼å‡ºå®Œæˆï¼")

if __name__ == "__main__":
    export()