"""
è®­ç»ƒè„šæœ¬ï¼šåŸºäº OT-CFM çš„é£æ ¼è¿ç§»æ¨¡å‹
ä½¿ç”¨æœ€ä¼˜ä¼ è¾“æµåŒ¹é… (Optimal Transport Conditional Flow Matching)

ä¼˜åŒ–æ–¹æ¡ˆï¼š
1. å…¨å†…å­˜æ•°æ®é›†ï¼ˆæ¶ˆé™¤IOç“¶é¢ˆï¼‰
2. ç§»é™¤æ‰€æœ‰æ‰‹åŠ¨æ˜¾å­˜ç®¡ç†ï¼ˆæ¶ˆé™¤GPUåŒæ­¥åœé¡¿ï¼‰
3. ä¼˜åŒ–å†…å­˜å¸ƒå±€ï¼ˆchannels_lasté¢„è½¬æ¢ï¼‰
4. Pin Memory + Non-blocking Transferï¼ˆCPU-GPUå¼‚æ­¥æµæ°´çº¿ï¼‰
5. Torch Compileï¼ˆæ¶ˆé™¤Pythonè§£é‡Šå™¨å¼€é”€ï¼Œç®—å­èåˆï¼‰
"""

import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import numpy as np
from pathlib import Path
import gc
from PIL import Image
import random
import logging
import csv
from datetime import datetime

from model import create_model


class ReflowDataset(Dataset):
    """
    Reflowç¼“å­˜æ•°æ®é›† - ç”¨äºstage2è®­ç»ƒ
    
    Reflowæ•°æ®åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š
    - x0: åˆå§‹å™ªå£°å‘é‡
    - x1: ç›®æ ‡å›¾åƒçš„latent
    
    ç”¨äºä¼˜åŒ–ODEæ±‚è§£è·¯å¾„ï¼Œæé«˜ç”Ÿæˆè´¨é‡
    """
    def __init__(self, reflow_data_root, num_styles=None):
        super().__init__()
        self.data_root = Path(reflow_data_root)
        
        if not self.data_root.exists():
            raise ValueError(f"Reflow data directory not found: {reflow_data_root}")
        
        print(f"\nâš¡ Loading Reflow dataset from {reflow_data_root}...")
        
        # è¯»å–æ‰€æœ‰reflowå¯¹
        pairs = []
        for pair_file in sorted(self.data_root.glob("pair_*.pt")):
            try:
                pair_data = torch.load(pair_file, map_location='cpu', weights_only=True)
                # pair_data åº”è¯¥åŒ…å« x0, x1, style_id
                pairs.append(pair_data)
            except Exception as e:
                print(f"âŒ Failed to load {pair_file}: {e}")
        
        if not pairs:
            raise ValueError(f"No reflow pairs found in {reflow_data_root}")
        
        print(f"âœ… Loaded {len(pairs)} reflow pairs")
        
        # å †å æ•°æ®
        self.x0_list = []
        self.x1_list = []
        self.style_ids = []
        
        for pair in pairs:
            if isinstance(pair, dict):
                self.x0_list.append(pair['x0'])
                self.x1_list.append(pair['x1'])
                self.style_ids.append(pair.get('style_id', 0))
            elif isinstance(pair, tuple):
                self.x0_list.append(pair[0])
                self.x1_list.append(pair[1])
                self.style_ids.append(pair[2] if len(pair) > 2 else 0)
        
        self.x0_tensor = torch.stack(self.x0_list).contiguous(memory_format=torch.channels_last)
        self.x1_tensor = torch.stack(self.x1_list).contiguous(memory_format=torch.channels_last)
        self.style_tensor = torch.tensor(self.style_ids, dtype=torch.long)
        
        memory_mb = (self.x0_tensor.numel() + self.x1_tensor.numel()) * 4 / (1024**2)
        print(f"Memory usage: {memory_mb:.2f} MB")
        print(f"Data shape: x0={self.x0_tensor.shape}, x1={self.x1_tensor.shape}")
    
    def __len__(self):
        return len(self.style_tensor)
    
    def __getitem__(self, idx):
        return {
            'x0': self.x0_tensor[idx],
            'x1': self.x1_tensor[idx],
            'style_id': self.style_tensor[idx]
        }


class InMemoryLatentDataset(Dataset):
    """
    å…¨å†…å­˜Latentæ•°æ®é›† - æé€Ÿç‰ˆ
    
    è®¾è®¡ç†å¿µï¼š
    - è®­ç»ƒå¼€å§‹å‰ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®åˆ°RAM
    - æ¶ˆé™¤è®­ç»ƒæ—¶çš„IOç“¶é¢ˆ
    - é¢„éªŒè¯æ•°æ®å°ºå¯¸ï¼Œé¿å…è¿è¡Œæ—¶æ’å€¼
    """
    def __init__(self, data_root, num_styles=None):
        super().__init__()
        self.data_root = Path(data_root)
        
        # è¯»å–å…ƒæ•°æ®
        metadata_path = self.data_root.parent / "wikiart_dataset" / "metadata.json"
        if metadata_path.exists():
            print(f"Loading metadata from {metadata_path}")
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            valid_styles = metadata["valid_styles"]
            style_to_id = metadata["style_to_id"]
            
            if num_styles is not None:
                max_style_id = max(style_to_id.values())
                if max_style_id >= num_styles:
                    raise ValueError(
                        f"Metadata contains style_id={max_style_id} but num_classes={num_styles}. "
                        f"Please update config.json 'num_classes' to {max_style_id + 1} or regenerate metadata."
                    )
        else:
            valid_styles = sorted([d.name for d in self.data_root.iterdir() if d.is_dir()])
            style_to_id = {s: i for i, s in enumerate(valid_styles)}
            print("âš  Metadata not found, using folder names for style mapping")
            
            if num_styles is not None and len(valid_styles) > num_styles:
                raise ValueError(
                    f"Found {len(valid_styles)} styles but num_classes={num_styles}. "
                    f"Update config.json or reduce the number of style folders."
                )
        
        self.style_to_id = style_to_id
        self.num_expected_styles = num_styles
        
        print("\nâš¡ Loading all latents into RAM for maximum training speed...")
        print("This may take 30-60 seconds but will eliminate all IO bottlenecks.\n")
        
        latents_list = []
        styles_list = []
        failed_files = []
        
        # é¢„åŠ è½½æ‰€æœ‰æ•°æ®
        for style_name in valid_styles:
            style_dir = self.data_root / style_name
            if not style_dir.exists():
                continue
            
            style_id = style_to_id[style_name]
            latent_files = list(style_dir.glob("*.pt"))
            
            for fpath in tqdm(latent_files, desc=f"Loading {style_name}", leave=False):
                try:
                    latent = torch.load(fpath, map_location='cpu', weights_only=True)
                    
                    # ä¸¥æ ¼éªŒè¯å°ºå¯¸ - å¦‚æœä¸æ˜¯32x32ï¼Œè·³è¿‡å¹¶è®°å½•
                    if latent.shape != (4, 32, 32):
                        failed_files.append((fpath, latent.shape))
                        continue
                    
                    latents_list.append(latent)
                    styles_list.append(style_id)
                    
                except Exception as e:
                    print(f"âŒ Failed to load {fpath}: {e}")
        
        if len(latents_list) == 0:
            raise ValueError(f"No valid latent files found in {data_root}!")
        
        # è½¬æ¢ä¸ºå¤§Tensorï¼ˆä¸€æ¬¡æ€§æ“ä½œï¼Œè®­ç»ƒæ—¶é›¶æ‹·è´ï¼‰
        print("\nğŸ“¦ Stacking tensors into single array...")
        self.latents_tensor = torch.stack(latents_list)  # [N, 4, 32, 32]
        self.styles_tensor = torch.tensor(styles_list, dtype=torch.long)  # [N]
        
        # é¢„è½¬æ¢ä¸ºchannels_lastæ ¼å¼ï¼ˆé¿å…è®­ç»ƒæ—¶è½¬æ¢ï¼‰
        self.latents_tensor = self.latents_tensor.contiguous(memory_format=torch.channels_last)
        
        # ç»Ÿè®¡ä¿¡æ¯
        memory_mb = self.latents_tensor.element_size() * self.latents_tensor.numel() / (1024**2)
        unique_styles = sorted(set(styles_list))
        
        print(f"\nâœ… Dataset loaded successfully!")
        print(f"   Total samples: {len(self.styles_tensor)}")
        print(f"   Unique styles: {unique_styles}")
        print(f"   Memory usage: {memory_mb:.2f} MB")
        print(f"   Tensor shape: {self.latents_tensor.shape}")
        
        if failed_files:
            print(f"\nâš ï¸  Warning: {len(failed_files)} files skipped due to wrong shape:")
            for fpath, shape in failed_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   - {fpath.name}: {shape} (expected [4, 32, 32])")
            if len(failed_files) > 5:
                print(f"   ... and {len(failed_files) - 5} more")
            print(f"\nğŸ’¡ Tip: Re-run preprocess_latents.py with target_size=256 to fix this.")
        
        if num_styles is not None and max(unique_styles) >= num_styles:
            raise ValueError(
                f"Dataset contains style_id={max(unique_styles)} but model expects num_classes={num_styles}"
            )
    
    def __len__(self):
        return len(self.styles_tensor)
    
    def __getitem__(self, idx):
        """æé€Ÿè¿”å› - æ— IOï¼Œæ— å˜æ¢"""
        return {
            'latent': self.latents_tensor[idx],  # å·²ç»æ˜¯ channels_last
            'style_id': self.styles_tensor[idx]
        }


class OTCFMTrainer:
    """
    OT-CFM è®­ç»ƒå™¨ - ä¼˜åŒ–ç‰ˆ
    
    å…³é”®ä¼˜åŒ–ï¼š
    1. ç§»é™¤æ‰€æœ‰torch.cuda.empty_cache()è°ƒç”¨ï¼ˆæ¶ˆé™¤GPUåŒæ­¥åœé¡¿ï¼‰
    2. ç§»é™¤æ‰‹åŠ¨delæ“ä½œï¼ˆäº¤ç»™PyTorchè‡ªåŠ¨ç®¡ç†ï¼‰
    3. ç®€åŒ–è®­ç»ƒå¾ªç¯ï¼ˆå‡å°‘CPUå¼€é”€ï¼‰
    """
    def __init__(self, config, model, device):
        self.config = config
        self.model = model
        self.device = device
        
        # ä»é…ç½®è¯»å– num_classes ç”¨äºéªŒè¯
        self.expected_num_classes = config['data']['num_classes']
        
        # è®­ç»ƒé…ç½®
        train_cfg = config['training']
        self.batch_size = train_cfg['batch_size']
        self.learning_rate = train_cfg['learning_rate']
        self.num_epochs = train_cfg.get('stage1_epochs', 200)
        self.use_amp = train_cfg.get('use_amp', True)
        self.label_drop_prob = train_cfg.get('label_drop_prob', 0.10)
        
        # CFGç­–ç•¥
        self.use_avg_style_for_uncond = train_cfg.get('use_avg_style_for_uncond', True)
        
        # åŠ¨æ€epsilon
        self.dynamic_epsilon = train_cfg.get('dynamic_epsilon', True)
        self.epsilon_warmup_epochs = train_cfg.get('epsilon_warmup_epochs', 100)
        self.current_epoch = 0
        
        # æ¨ç†é…ç½®
        self.eval_step = train_cfg.get('eval_step', 10)
        self.inference_cfg = config.get('inference', {})
        
        # æ–­ç‚¹ç»­ä¼ é…ç½®
        self.resume_checkpoint = train_cfg.get('resume_checkpoint', '')
        self.save_interval = train_cfg.get('save_interval', 10)
        
        # Optimizer
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=self.learning_rate,
            weight_decay=1e-5,
            betas=(0.9, 0.999)
        )
        
        # LR Scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.num_epochs,
            eta_min=1e-6
        )
        
        # AMP Scaler
        self.scaler = torch.amp.GradScaler('cuda', enabled=self.use_amp)
        
        # Checkpoint
        self.save_dir = Path(config['checkpoint']['save_dir'])
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨ç†è¾“å‡ºç›®å½•
        self.inference_dir = self.save_dir / "inference"
        self.inference_dir.mkdir(exist_ok=True)
        
        # VAE è§£ç å™¨ï¼ˆæ¨ç†ç”¨ï¼‰
        self.vae = None
        self._init_vae()
        
        # æ•°æ®é›†å¼•ç”¨ï¼ˆç¨ååœ¨trainæ–¹æ³•ä¸­åˆå§‹åŒ–ï¼‰
        self.dataset = None
        
        # è®°å½•èµ·å§‹epoch
        self.start_epoch = 1
        
        # æ—¥å¿—ç³»ç»Ÿ
        self._init_logging()
    
    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ"""
        # åˆ›å»ºlogsç›®å½•
        self.log_dir = self.save_dir / "logs"
        self.log_dir.mkdir(exist_ok=True)
        
        # æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ–‡æœ¬æ—¥å¿—æ–‡ä»¶
        self.log_file = self.log_dir / f"training_{timestamp}.log"
        self.csv_file = self.log_dir / f"training_{timestamp}.csv"
        
        # é…ç½®logging
        logging.basicConfig(
            level=logging.INFO,
            format='[%(asctime)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            handlers=[
                logging.FileHandler(self.log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # CSVæ—¥å¿—å¤´
        self.csv_headers = [
            'epoch', 'stage', 'avg_loss', 'learning_rate', 'epsilon',
            'inference_time', 'checkpoint_saved', 'notes'
        ]
        with open(self.csv_file, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=self.csv_headers)
            writer.writeheader()
    
    def _log_training_info(self, epoch, avg_loss, learning_rate, epsilon, inference_time=None):
        """
        è®°å½•è®­ç»ƒä¿¡æ¯åˆ°æ—¥å¿—
        
        Args:
            epoch: å½“å‰epoch
            avg_loss: å¹³å‡loss
            learning_rate: å­¦ä¹ ç‡
            epsilon: å½“å‰epsilonå€¼
            inference_time: æ¨ç†è€—æ—¶ï¼ˆç§’ï¼‰
        """
        msg = f"Epoch {epoch:3d}/{self.num_epochs} | Loss: {avg_loss:.6f} | LR: {learning_rate:.2e} | Îµ: {epsilon:.4f}"
        if inference_time:
            msg += f" | Inference: {inference_time:.1f}s"
        self.logger.info(msg)
        
        # å†™å…¥CSV
        row = {
            'epoch': epoch,
            'stage': 'stage1',
            'avg_loss': f"{avg_loss:.6f}",
            'learning_rate': f"{learning_rate:.2e}",
            'epsilon': f"{epsilon:.4f}",
            'inference_time': f"{inference_time:.1f}s" if inference_time else "â€”",
            'checkpoint_saved': 'âœ“',
            'notes': ''
        }
        with open(self.csv_file, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=self.csv_headers)
            writer.writerow(row)
    
    def _init_vae(self):
        """åˆå§‹åŒ– VAE ç”¨äºè§£ç """
        try:
            from diffusers import AutoencoderKL
            print("Loading VAE decoder for inference...")
            self.vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse")
            self.vae = self.vae.to(self.device)
            self.vae.eval()
            print("âœ“ VAE decoder loaded")
        except Exception as e:
            print(f"âš ï¸  Failed to load VAE: {e}")
            print("   Inference will only save latent files")
            self.vae = None
    
    def _latent_to_image(self, latent_tensor):
        """
        å°†latentè§£ç ä¸ºå›¾ç‰‡
        
        Args:
            latent_tensor: [B, 4, H, W] latent å¼ é‡
        
        Returns:
            PIL Image æˆ– None
        """
        if self.vae is None:
            return None
        
        try:
            with torch.no_grad():
                # è§£ç ï¼šlatent -> å›¾ç‰‡
                decoded = self.vae.decode(latent_tensor / 0.18215).sample
                # è½¬åˆ° [0, 1]
                decoded = (decoded + 1.0) / 2.0
                decoded = torch.clamp(decoded, 0, 1)
                
                # è½¬ä¸º PIL Image
                img = decoded[0].cpu().permute(1, 2, 0).numpy()
                img = (img * 255).astype(np.uint8)
                return Image.fromarray(img)
        except Exception as e:
            print(f"   Error decoding latent: {e}")
            return None
    
    def _prepare_inference_samples(self):
        """ä»æ•°æ®é›†ä¸­ä¸ºæ¯ä¸ªé£æ ¼ç±»åˆ«é€‰æ‹©ä¸€å¼ ä»£è¡¨å›¾ç‰‡ç”¨äºæ¨ç†"""
        # ä½¿ç”¨å·²åŠ è½½çš„æ•°æ®é›†ï¼Œè€Œä¸æ˜¯ä»ç£ç›˜è¯»å–
        if not hasattr(self, 'dataset') or self.dataset is None:
            return  # å¦‚æœæ•°æ®é›†è¿˜æœªå‡†å¤‡ï¼Œå»¶è¿Ÿåˆå§‹åŒ–
        
        self.inference_samples = {}
        unique_styles = set()
        
        # ä»å†…å­˜æ•°æ®é›†ä¸­ä¸ºæ¯ä¸ªstyleæ‰¾ä¸€ä¸ªæ ·æœ¬
        for idx in range(len(self.dataset)):
            item = self.dataset[idx]
            style_id = item['style_id'].item()
            
            # æ‰¾åˆ°style_idå¯¹åº”çš„åç§°
            for style_name, sid in self.dataset.style_to_id.items():
                if sid == style_id and style_name not in unique_styles:
                    latent = item['latent']
                    self.inference_samples[style_name] = {
                        'latent': latent,
                        'style_id': style_id
                    }
                    unique_styles.add(style_name)
                    break
            
            if len(unique_styles) == len(self.dataset.style_to_id):
                break  # å·²æ”¶é›†æ‰€æœ‰style
        
        if self.inference_samples:
            print(f"âœ“ Prepared {len(self.inference_samples)} inference samples")
    
    def load_checkpoint(self, checkpoint_path):
        """
        åŠ è½½checkpointè¿›è¡Œæ–­ç‚¹ç»­ä¼ 
        
        Args:
            checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        checkpoint_path = Path(checkpoint_path)
        
        if not checkpoint_path.exists():
            self.logger.error(f"Checkpoint not found: {checkpoint_path}")
            return False
        
        try:
            self.logger.info(f"ğŸ“¥ Loading checkpoint: {checkpoint_path.name}")
            checkpoint = torch.load(str(checkpoint_path), map_location=self.device)
            
            # åŠ è½½æ¨¡å‹æƒé‡
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.logger.info("  âœ“ Model state loaded")
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.logger.info("  âœ“ Optimizer state loaded")
            
            # åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
            if 'scheduler_state_dict' in checkpoint:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                self.logger.info("  âœ“ Scheduler state loaded")
            
            # åŠ è½½AMP scalerçŠ¶æ€
            if 'scaler_state_dict' in checkpoint and self.use_amp:
                self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
                self.logger.info("  âœ“ AMP Scaler state loaded")
            
            # æ¢å¤è®­ç»ƒè¿›åº¦
            self.start_epoch = checkpoint.get('epoch', 0) + 1
            self.current_epoch = checkpoint.get('epoch', 0)
            
            self.logger.info(f"âœ… Resume from epoch {self.start_epoch}")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to load checkpoint: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    @torch.no_grad()
    def run_inference(self, epoch):
        """
        è¿è¡Œæ¨ç†ï¼šæ¯ä¸ªæºå›¾ç‰‡è½¬æ¢åˆ°æ‰€æœ‰ç›®æ ‡é£æ ¼
        ä½¿ç”¨OTé€»è¾‘ï¼šä»æºlatenté€šè¿‡é£æ ¼è½¬æ¢åœºåˆ°è¾¾ç›®æ ‡é£æ ¼
        
        Args:
            epoch: å½“å‰epochæ•°
        """
        if not self.inference_samples:
            self.logger.warning("Skip inference: no samples prepared")
            return
        
        import time
        inference_start = time.time()
        
        self.model.eval()
        
        self.logger.info(f"{'='*60}")
        self.logger.info(f"ğŸ¨ Running Inference at Epoch {epoch}")
        self.logger.info(f"{'='*60}")
        
        # æ¨ç†å‚æ•°
        num_steps = self.inference_cfg.get('num_inference_steps', 15)
        cfg_scale = self.inference_cfg.get('cfg_scale', 2.0)
        use_cfg = self.inference_cfg.get('use_cfg', True)
        
        # ä¸ºæœ¬æ¬¡epochåˆ›å»ºå­ç›®å½•
        epoch_dir = self.inference_dir / f"epoch_{epoch:04d}"
        epoch_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡æ¨ç†ç»“æœ
        total_generated = 0
        
        # éå†æ¯ä¸ªæºå›¾ç‰‡
        for src_style, src_data in self.inference_samples.items():
            self.logger.info(f"ğŸ“· Source: {src_style}")
            
            # ç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„latent
            src_latent = src_data['latent'].unsqueeze(0).to(self.device, memory_format=torch.channels_last)
            src_style_id = src_data['style_id']
            
            # ä¿å­˜åŸå§‹å›¾ç‰‡
            src_img = self._latent_to_image(src_latent)
            if src_img is not None:
                src_img.save(str(epoch_dir / f"00_src_{src_style}.png"))
                self.logger.info(f"  ğŸ“¸ Source image: 00_src_{src_style}.png")
            
            # è½¬æ¢åˆ°æ¯ä¸ªç›®æ ‡é£æ ¼ï¼ˆåŒ…æ‹¬è‡ªèº«ï¼‰
            for tgt_style, tgt_data in self.inference_samples.items():
                tgt_style_id = tgt_data['style_id']
                
                # OT-CFMçš„æ­£ç¡®æ¨ç†ï¼šä»æºlatentå¼€å§‹ï¼Œç»è¿‡velocity fieldè¿›è¡Œé£æ ¼è½¬æ¢
                # x(t=0) = x_srcï¼Œé€šè¿‡tä»0åˆ°1çš„ç§¯åˆ†ï¼Œå¾—åˆ°x(t=1)åœ¨ç›®æ ‡é£æ ¼ç©ºé—´ä¸­çš„å¯¹åº”
                x = src_latent.clone()  # ä»æºlatentå¼€å§‹ï¼è€Œä¸æ˜¯éšæœºå™ªå£°
                
                # ODEæ±‚è§£
                dt = 1.0 / num_steps
                tgt_id_tensor = torch.tensor([tgt_style_id], dtype=torch.long, device=self.device)
                
                for step in range(num_steps):
                    t = torch.full((1,), step * dt, device=self.device)
                    
                    if use_cfg:
                        # Classifier-Free Guidance
                        # ä¸ºäº†é¿å…torch.compileçš„CUDA Graphå¤ç”¨é—®é¢˜ï¼Œæ˜¾å¼å…‹éš†è¾“å‡º
                        v_cond = self.model(x, t, tgt_id_tensor, use_avg_style=False).clone()
                        
                        # æ ‡è®°CUDA Graphçš„æ–°æ­¥éª¤
                        if hasattr(torch.compiler, 'cudagraph_mark_step_begin'):
                            torch.compiler.cudagraph_mark_step_begin()
                        
                        v_uncond = self.model(x, t, tgt_id_tensor, use_avg_style=True)
                        v = v_uncond + cfg_scale * (v_cond - v_uncond)
                    else:
                        v = self.model(x, t, tgt_id_tensor, use_avg_style=False)
                    
                    x = x + v * dt
                
                # ä¿å­˜ç”Ÿæˆçš„å›¾ç‰‡
                gen_img = self._latent_to_image(x)
                if gen_img is not None:
                    gen_img.save(str(epoch_dir / f"{src_style}_to_{tgt_style}.png"))
                    total_generated += 1
        
        inference_time = time.time() - inference_start
        self.logger.info(f"âœ… Inference completed in {inference_time:.1f}s ({total_generated} images)")
        self.logger.info(f"{'='*60}")
        
        self.model.train()
        
        return inference_time
    
    def get_dynamic_epsilon(self):
        """åŠ¨æ€epsilonè°ƒæ•´"""
        if not self.dynamic_epsilon:
            return 0.0
        epsilon = min(0.1, self.current_epoch / self.epsilon_warmup_epochs)
        return epsilon
    
    def compute_otcfm_loss(self, x1, style_id):
        """è®¡ç®— OT-CFM æŸå¤±"""
        batch_size = x1.size(0)
        
        # 1. é‡‡æ · x0 ~ N(0, I)
        x0 = torch.randn_like(x1)
        
        # 2. é‡‡æ ·æ—¶é—´ t ~ Uniform(Îµ, 1)
        epsilon = self.get_dynamic_epsilon()
        t = torch.rand(batch_size, device=self.device) * (1.0 - epsilon) + epsilon
        
        # 3. æ„é€ è·¯å¾„ x_t = (1-t)*x0 + t*x1
        t_expanded = t[:, None, None, None]
        x_t = (1 - t_expanded) * x0 + t_expanded * x1
        
        # 4. è®¡ç®—ç›®æ ‡é€Ÿåº¦åœº u_t = x1 - x0
        u_t = x1 - x0
        
        # 5. Label Dropping for CFG
        drop_mask = torch.rand(batch_size, device=self.device) < self.label_drop_prob
        
        if self.use_avg_style_for_uncond and drop_mask.any():
            v_pred = self.model(x_t, t, style_id, use_avg_style=False)
            
            if drop_mask.sum() > 0:
                x_t_drop = x_t[drop_mask]
                t_drop = t[drop_mask]
                style_id_drop = style_id[drop_mask]
                v_pred_drop = self.model(x_t_drop, t_drop, style_id_drop, use_avg_style=True)
                v_pred[drop_mask] = v_pred_drop
        else:
            v_pred = self.model(x_t, t, style_id, use_avg_style=False)
        
        # 6. MSE Loss
        loss = F.mse_loss(v_pred, u_t)
        return loss
    
    def train_epoch(self, dataloader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch - æè‡´ä¼˜åŒ–ç‰ˆ"""
        self.model.train()
        self.current_epoch = epoch
        
        total_loss = 0
        epsilon = self.get_dynamic_epsilon()
        
        # leave=False ä¿æŒæ§åˆ¶å°æ¸…çˆ½
        pbar = tqdm(dataloader, desc=f"Epoch {epoch}/{self.num_epochs}", leave=False)
        
        for batch in pbar:
            # non_blocking=True å®ç° CPU-GPU å¼‚æ­¥ä¼ è¾“
            latent = batch['latent'].to(self.device, non_blocking=True)
            style_id = batch['style_id'].to(self.device, non_blocking=True)
            
            # éªŒè¯èŒƒå›´
            if style_id.max().item() >= self.expected_num_classes:
                raise ValueError(
                    f"Batch contains style_id={style_id.max().item()} but expected num_classes={self.expected_num_classes}"
                )
            
            # æ•°æ®å·²ç»æ˜¯ channels_lastï¼Œæ— éœ€è½¬æ¢
            
            # è®­ç»ƒæ­¥éª¤
            self.optimizer.zero_grad(set_to_none=True)
            
            with torch.amp.autocast('cuda', enabled=self.use_amp, dtype=torch.bfloat16):
                loss = self.compute_otcfm_loss(latent, style_id)
            
            self.scaler.scale(loss).backward()
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            # è®°å½•
            total_loss += loss.item()
            pbar.set_postfix({'loss': f"{loss.item():.4f}", 'Îµ': f"{epsilon:.3f}"})
            
            # ğŸŸ¢ ç§»é™¤æ‰€æœ‰æ‰‹åŠ¨å†…å­˜ç®¡ç†ï¼š
            # - ä¸è¦ del latent, style_id, loss
            # - ä¸è¦ torch.cuda.empty_cache()
            # - ä¸è¦ gc.collect()
            # PyTorch ä¼šè‡ªåŠ¨ç®¡ç†è¿™äº›ï¼
        
        avg_loss = total_loss / len(dataloader)
        return avg_loss
    
    def train(self, dataloader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        import time
        train_start_time = time.time()
        
        self.logger.info("="*80)
        self.logger.info("ğŸš€ OT-CFM Training Started")
        self.logger.info("="*80)
        self.logger.info(f"Device: {self.device}")
        self.logger.info(f"GPU: {torch.cuda.get_device_name(0) if self.device.type == 'cuda' else 'CPU'}")
        self.logger.info(f"Batch size: {self.batch_size}")
        self.logger.info(f"Learning rate: {self.learning_rate}")
        self.logger.info(f"AMP: {self.use_amp}")
        self.logger.info(f"Compile: {self.model.__class__.__name__}")
        self.logger.info(f"Dataset size: {len(dataloader.dataset)}")
        self.logger.info(f"Batches per epoch: {len(dataloader)}")
        self.logger.info("="*80)
        
        # å°è¯•åŠ è½½æ–­ç‚¹
        checkpoint_path = Path(self.resume_checkpoint) if self.resume_checkpoint else None
        if checkpoint_path and checkpoint_path.exists():
            # å¦‚æœæŒ‡å®šäº†checkpointè·¯å¾„ä¸”å­˜åœ¨ï¼ŒåŠ è½½å®ƒ
            self.load_checkpoint(checkpoint_path)
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„checkpoint
            ckpt_files = sorted(self.save_dir.glob("stage1_epoch*.pt"))
            if ckpt_files:
                latest_ckpt = ckpt_files[-1]
                self.logger.info(f"ğŸ“‚ Found latest checkpoint: {latest_ckpt.name}")
                self.load_checkpoint(latest_ckpt)
            else:
                self.logger.info("No checkpoint found, starting fresh training")
        
        # ä¿å­˜datasetå¼•ç”¨å¹¶åˆå§‹åŒ–inferenceæ ·æœ¬
        self.dataset = dataloader.dataset
        self._prepare_inference_samples()
        
        # åˆå§‹åŒ–å¹³å‡é£æ ¼åµŒå…¥
        if self.start_epoch == 1:
            self.logger.info("Initializing average style embedding...")
            self.model.initialize_avg_style_embedding()
            self.logger.info("âœ“ Average style embedding initialized")
        else:
            self.logger.info(f"â­ï¸  Resuming from epoch {self.start_epoch}")
        
        self.logger.info("")
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.start_epoch, self.num_epochs + 1):
            epoch_start = time.time()
            avg_loss = self.train_epoch(dataloader, epoch)
            epoch_time = time.time() - epoch_start
            
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            epsilon = self.get_dynamic_epsilon()
            
            # è®°å½•è®­ç»ƒä¿¡æ¯
            self._log_training_info(epoch, avg_loss, current_lr, epsilon)
            
            # å®šæœŸæ¨ç†
            inference_time = None
            if epoch % self.eval_step == 0 or epoch == self.num_epochs:
                inference_time = self.run_inference(epoch)
                self.logger.info("")
            
            # ä¿å­˜checkpoint
            if epoch % self.save_interval == 0 or epoch == self.num_epochs:
                self.save_checkpoint(epoch)
                self.logger.info("")
        
        # ä¿å­˜final checkpoint
        final_path = self.save_dir / "stage1_final.pt"
        if not final_path.exists():
            self.save_checkpoint(self.num_epochs, is_final=True)
        
        # æ€»ç»“
        total_time = time.time() - train_start_time
        hours, remainder = divmod(total_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        
        self.logger.info("="*80)
        self.logger.info(f"âœ… Stage1 Training completed!")
        self.logger.info(f"Total time: {int(hours)}h {int(minutes)}m {int(seconds)}s")
        self.logger.info(f"Checkpoint dir: {self.save_dir}")
        self.logger.info(f"Logs: {self.log_dir}")
        self.logger.info("="*80)
    
    def train_stage2(self, reflow_dataloader):
        """Stage2: Reflow è®­ç»ƒ - åŸºäºstage1æ¨¡å‹è¿›è¡Œé‡æµç¨‹ä¼˜åŒ–"""
        import time
        stage2_start_time = time.time()
        
        self.logger.info("")
        self.logger.info("="*80)
        self.logger.info("ğŸš€ OT-CFM Stage2 (Reflow) Training Started")
        self.logger.info("="*80)
        
        # è·å–stage2é…ç½®
        train_cfg = self.config['training']
        stage2_epochs = train_cfg.get('stage2_epochs', 50)
        stage2_lr = train_cfg.get('stage2_learning_rate', self.learning_rate * 0.1)
        
        # è°ƒæ•´ä¸ºstage2é…ç½®
        self.num_epochs = stage2_epochs
        self.start_epoch = 1
        self.current_epoch = 0
        
        # é‡ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=stage2_lr,
            weight_decay=1e-5,
            betas=(0.9, 0.999)
        )
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=stage2_epochs,
            eta_min=1e-6
        )
        
        self.logger.info(f"Device: {self.device}")
        self.logger.info(f"Batch size: {self.batch_size}")
        self.logger.info(f"Stage2 Learning rate: {stage2_lr}")
        self.logger.info(f"Dataset size: {len(reflow_dataloader.dataset)}")
        self.logger.info(f"Batches per epoch: {len(reflow_dataloader)}")
        self.logger.info("="*80)
        
        self.logger.info("")
        
        # Stage2è®­ç»ƒå¾ªç¯
        for epoch in range(1, stage2_epochs + 1):
            epoch_start = time.time()
            avg_loss = self.train_epoch(reflow_dataloader, epoch)
            epoch_time = time.time() - epoch_start
            
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            epsilon = self.get_dynamic_epsilon()
            
            # è®°å½•è®­ç»ƒä¿¡æ¯ï¼ˆæ ‡è®°ä¸ºstage2ï¼‰
            msg = f"Epoch {epoch:3d}/{stage2_epochs} | Loss: {avg_loss:.6f} | LR: {current_lr:.2e} | Îµ: {epsilon:.4f}"
            self.logger.info(msg)
            
            # å†™å…¥CSV
            row = {
                'epoch': epoch,
                'stage': 'stage2',
                'avg_loss': f"{avg_loss:.6f}",
                'learning_rate': f"{current_lr:.2e}",
                'epsilon': f"{epsilon:.4f}",
                'inference_time': "â€”",
                'checkpoint_saved': 'âœ“' if epoch % train_cfg.get('stage2_save_interval', 5) == 0 else 'â€”',
                'notes': ''
            }
            with open(self.csv_file, 'a', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=self.csv_headers)
                writer.writerow(row)
            
            # å®šæœŸæ¨ç†
            if epoch % self.eval_step == 0 or epoch == stage2_epochs:
                inference_time = self.run_inference(epoch)
                self.logger.info("")
            
            # ä¿å­˜stage2 checkpoint
            if epoch % train_cfg.get('stage2_save_interval', 5) == 0 or epoch == stage2_epochs:
                stage2_ckpt_path = self.save_dir / f"stage2_epoch{epoch}.pt"
                checkpoint = {
                    'epoch': epoch,
                    'stage': 2,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict(),
                    'scaler_state_dict': self.scaler.state_dict() if self.use_amp else None,
                    'config': self.config,
                    'training_info': {
                        'current_epoch': epoch,
                        'learning_rate': current_lr,
                        'epsilon': epsilon,
                    }
                }
                torch.save(checkpoint, str(stage2_ckpt_path))
                file_size_mb = stage2_ckpt_path.stat().st_size / (1024 ** 2)
                self.logger.info(f"ğŸ’¾ Stage2 checkpoint saved: {stage2_ckpt_path.name} ({file_size_mb:.1f}MB)")
                self.logger.info("")
        
        # ä¿å­˜final stage2 checkpoint
        final_stage2_path = self.save_dir / "stage2_final.pt"
        checkpoint = {
            'epoch': stage2_epochs,
            'stage': 2,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'scaler_state_dict': self.scaler.state_dict() if self.use_amp else None,
            'config': self.config,
            'training_info': {
                'current_epoch': stage2_epochs,
                'learning_rate': self.optimizer.param_groups[0]['lr'],
                'epsilon': self.get_dynamic_epsilon(),
            }
        }
        torch.save(checkpoint, str(final_stage2_path))
        self.logger.info(f"ğŸ’¾ Final stage2 checkpoint saved: {final_stage2_path.name}")
        
        # æ€»ç»“
        total_time = time.time() - stage2_start_time
        hours, remainder = divmod(total_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        
        self.logger.info("="*80)
        self.logger.info(f"âœ… Stage2 (Reflow) Training completed!")
        self.logger.info(f"Total time: {int(hours)}h {int(minutes)}m {int(seconds)}s")
        self.logger.info("="*80)
    
    def save_checkpoint(self, epoch, is_final=False):
        """
        ä¿å­˜checkpoint
        
        Args:
            epoch: å½“å‰epochæ•°
            is_final: æ˜¯å¦ä¸ºæœ€ç»ˆcheckpoint
        """
        if is_final:
            checkpoint_path = self.save_dir / "stage1_final.pt"
        else:
            checkpoint_path = self.save_dir / f"stage1_epoch{epoch}.pt"
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'scaler_state_dict': self.scaler.state_dict() if self.use_amp else None,
            'config': self.config,
            'training_info': {
                'current_epoch': epoch,
                'learning_rate': self.optimizer.param_groups[0]['lr'],
                'epsilon': self.get_dynamic_epsilon(),
            }
        }
        
        # è½¬ä¸ºå­—ç¬¦ä¸²ç¡®ä¿å…¼å®¹æ€§
        torch.save(checkpoint, str(checkpoint_path))
        file_size_mb = checkpoint_path.stat().st_size / (1024 ** 2)
        self.logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path.name} ({file_size_mb:.1f}MB)")
        
        if not is_final:
            self._cleanup_old_checkpoints(keep_last=3)
    
    def _cleanup_old_checkpoints(self, keep_last=3):
        """
        æ¸…ç†æ—§çš„checkpointæ–‡ä»¶ï¼Œåªä¿ç•™æœ€è¿‘çš„Nä¸ª
        
        Args:
            keep_last: ä¿ç•™æœ€è¿‘çš„å‡ ä¸ªcheckpoint
        """
        ckpt_files = sorted(
            self.save_dir.glob("stage1_epoch*.pt"),
            key=lambda x: int(x.stem.split('epoch')[-1])
        )
        
        if len(ckpt_files) <= keep_last:
            return  # æ–‡ä»¶ä¸è¶³ï¼Œæ— éœ€æ¸…ç†
        
        # ä¿ç•™æœ€åNä¸ªå³å¯
        to_remove = ckpt_files[:-keep_last]
        
        for ckpt in to_remove:
            try:
                ckpt.unlink()
            except Exception:
                pass  # é™é»˜å¤±è´¥ï¼Œä¸å½±å“è®­ç»ƒ


def main():
    # åŠ è½½é…ç½®
    with open('config.json', 'r') as f:
        config = json.load(f)
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    if device.type == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # åº•å±‚ä¼˜åŒ–
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # ========== Stage1: OT-CFM Training ==========
    print("\n" + "="*80)
    print("STAGE1: OT-CFM Training")
    print("="*80)
    
    data_cfg = config['data']
    train_cfg = config['training']
    
    # Stage1: ä½¿ç”¨latentæ•°æ®é›†
    print("\nLoading Stage1 dataset...")
    dataset_stage1 = InMemoryLatentDataset(
        data_root=data_cfg['data_root'],
        num_styles=data_cfg['num_classes']
    )
    print(f"Dataset size: {len(dataset_stage1)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nCreating model...")
    model = create_model(config).to(device)
    
    num_params = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"Model parameters: {num_params:.2f}M")
    
    expected_num_classes = data_cfg['num_classes']
    print(f"Model configured for num_classes: {expected_num_classes}")
    
    # Torch Compile
    if train_cfg.get('use_compile', True):
        try:
            print("\nğŸš€ Compiling model with torch.compile...")
            print("   This may take 1-2 minutes on first run but will be cached.")
            model = torch.compile(model, mode="reduce-overhead")
            print("âœ… Model compiled successfully!")
        except Exception as e:
            print(f"âš ï¸  Torch compile failed: {e}")
            print("   Continuing with eager mode (no performance loss if PyTorch < 2.0)")
    else:
        print("âœ“ Model ready (native PyTorch eager mode)")
    
    # DataLoader - Stage1
    dataloader_stage1 = DataLoader(
        dataset_stage1,
        batch_size=train_cfg['batch_size'],
        shuffle=True,
        num_workers=0,
        pin_memory=True,
        drop_last=True
    )
    
    print(f"Batches per epoch: {len(dataloader_stage1)}")
    
    # è®­ç»ƒå™¨
    trainer = OTCFMTrainer(config, model, device)
    
    # Stage1è®­ç»ƒ
    trainer.train(dataloader_stage1)
    
    # ========== Stage2: Reflow Training ==========
    stage2_enabled = train_cfg.get('enable_stage2', False)
    
    if stage2_enabled:
        print("\n" + "="*80)
        print("STAGE2: Reflow Training")
        print("="*80)
        
        reflow_data_dir = train_cfg.get('reflow_data_dir', 'data_reflow_cache')
        
        try:
            # åŠ è½½reflowæ•°æ®é›†
            print(f"\nLoading Stage2 (Reflow) dataset...")
            dataset_stage2 = ReflowDataset(
                reflow_data_root=reflow_data_dir,
                num_styles=data_cfg['num_classes']
            )
            
            # DataLoader - Stage2
            dataloader_stage2 = DataLoader(
                dataset_stage2,
                batch_size=train_cfg['batch_size'],
                shuffle=True,
                num_workers=0,
                pin_memory=True,
                drop_last=True
            )
            
            # Stage2è®­ç»ƒ
            trainer.train_stage2(dataloader_stage2)
            
            print("\n" + "="*80)
            print("âœ… Both Stage1 and Stage2 training completed!")
            print("="*80)
            
        except Exception as e:
            print(f"\nâš ï¸  Stage2 skipped: {e}")
            print("   Check that reflow data directory exists and contains pair_*.pt files")
    else:
        print("\nâ„¹ï¸  Stage2 disabled in config. Set 'enable_stage2': true to enable reflow training.")


if __name__ == "__main__":
    main()
