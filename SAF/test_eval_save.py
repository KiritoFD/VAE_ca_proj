"""
æµ‹è¯•è¯„ä¼°å™¨çš„ä¿å­˜åŠŸèƒ½
éªŒè¯ä¸‰ä¸ªæŒ‡æ ‡æ˜¯å¦æ­£ç¡®ä¿å­˜ä¸º JSON æ–‡ä»¶
"""
import json
from pathlib import Path
import sys

def test_evaluation_files(eval_dir):
    """
    æ£€æŸ¥è¯„ä¼°ç›®å½•ä¸­çš„æ–‡ä»¶
    """
    eval_dir = Path(eval_dir)
    
    print("\n" + "="*60)
    print(f"ğŸ“‚ æ£€æŸ¥è¯„ä¼°ç›®å½•: {eval_dir}")
    print("="*60)
    
    if not eval_dir.exists():
        print("âŒ è¯„ä¼°ç›®å½•ä¸å­˜åœ¨ï¼")
        return False
    
    # æ£€æŸ¥ä¸‰ä¸ªå¿…éœ€çš„æ–‡ä»¶
    required_files = [
        "lpips_results.json",
        "clip_results.json", 
        "vgg_results.json",
        "metrics_summary.json"
    ]
    
    all_good = True
    
    for filename in required_files:
        filepath = eval_dir / filename
        if filepath.exists():
            print(f"âœ… {filename} å­˜åœ¨")
            
            # è¯»å–å¹¶éªŒè¯ JSON æ ¼å¼
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # éªŒè¯æ•°æ®ç»“æ„
                if "metric" in data:
                    print(f"   â””â”€ Metric: {data.get('metric')}")
                    if "overall_average" in data:
                        print(f"   â””â”€ Overall averages: {list(data['overall_average'].keys())}")
                elif "metrics" in data:
                    print(f"   â””â”€ Contains metrics: {list(data['metrics'].keys())}")
                    
            except json.JSONDecodeError as e:
                print(f"   âŒ JSON æ ¼å¼é”™è¯¯: {e}")
                all_good = False
            except Exception as e:
                print(f"   âš ï¸  è¯»å–é”™è¯¯: {e}")
        else:
            print(f"âŒ {filename} ä¸å­˜åœ¨ï¼")
            all_good = False
    
    print("="*60)
    
    if all_good:
        print("âœ… æ‰€æœ‰è¯„ä¼°æ–‡ä»¶éƒ½æ­£ç¡®ä¿å­˜ï¼")
        
        # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
        summary_path = eval_dir / "metrics_summary.json"
        if summary_path.exists():
            with open(summary_path, 'r', encoding='utf-8') as f:
                summary = json.load(f)
            
            print("\nğŸ“Š è¯„ä¼°æ±‡æ€»ï¼š")
            print(f"å®éªŒåç§°: {summary.get('experiment_name', 'N/A')}")
            print(f"è¯„ä¼°æ—¶é—´: {summary.get('evaluation_time', 'N/A')}")
            
            metrics = summary.get('metrics', {})
            for metric_name, metric_data in metrics.items():
                if isinstance(metric_data, dict) and 'overall_average' in metric_data:
                    print(f"\n{metric_name.upper()}:")
                    for style, values in metric_data['overall_average'].items():
                        if 'mean' in values:
                            print(f"  {style}: {values['mean']:.6f}")
    else:
        print("âŒ éƒ¨åˆ†è¯„ä¼°æ–‡ä»¶ç¼ºå¤±æˆ–æŸåï¼")
    
    return all_good


def find_latest_evaluation():
    """
    æŸ¥æ‰¾æœ€æ–°çš„è¯„ä¼°ç»“æœç›®å½•
    """
    results_root = Path("AutoSearch_Results")
    
    if not results_root.exists():
        print("âŒ AutoSearch_Results ç›®å½•ä¸å­˜åœ¨")
        return None
    
    # æŸ¥æ‰¾æ‰€æœ‰å®éªŒçš„ evaluation ç›®å½•
    eval_dirs = list(results_root.glob("*/checkpoints/evaluation"))
    
    if not eval_dirs:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•è¯„ä¼°ç›®å½•")
        return None
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
    eval_dirs.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    
    print(f"\næ‰¾åˆ° {len(eval_dirs)} ä¸ªè¯„ä¼°ç›®å½•")
    print(f"æœ€æ–°çš„è¯„ä¼°ç›®å½•: {eval_dirs[0]}")
    
    return eval_dirs[0]


if __name__ == "__main__":
    if len(sys.argv) > 1:
        # æŒ‡å®šç›®å½•
        test_dir = sys.argv[1]
        test_evaluation_files(test_dir)
    else:
        # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„
        latest = find_latest_evaluation()
        if latest:
            test_evaluation_files(latest)
        else:
            print("\nä½¿ç”¨æ–¹æ³•:")
            print("  python test_eval_save.py [è¯„ä¼°ç›®å½•è·¯å¾„]")
            print("  æˆ–è€…ä¸å¸¦å‚æ•°è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„è¯„ä¼°ç»“æœ")
