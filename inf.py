import torch
import torch.nn as nn
from pathlib import Path
from PIL import Image
import numpy as np

from SAFlow import SAFModel
from config import Config


@torch.no_grad()
def generate_style_transfer_cfg(
    content_latent, 
    target_style_id, 
    model, 
    device, 
    steps=25, 
    cfg_scale=4.0
):
    """
    SA-Flow v2 æ¨ç†ï¼šä½¿ç”¨ CFG (Classifier-Free Guidance) è¿›è¡Œé”åŒ–ç”Ÿæˆ
    
    Args:
        content_latent: [1, 4, 64, 64] - å†…å®¹å›¾çš„latent
        target_style_id: int - ç›®æ ‡é£æ ¼ID
        model: SAFModel
        device: cuda/cpu
        steps: int - ç§¯åˆ†æ­¥æ•°
        cfg_scale: float - CFG å¼ºåº¦ (3.0-5.0 æ¨è)
    
    Returns:
        stylized_latent: [1, 4, 64, 64]
    """
    model.eval()
    content_latent = content_latent.to(device)
    style_tensor = torch.tensor([target_style_id], dtype=torch.long, device=device)
    
    # ğŸ”´ v2: ä»çº¯å™ªå£°å¼€å§‹ (ä¸è®­ç»ƒä¸€è‡´)
    x_t = torch.randn_like(content_latent)
    
    # å‡†å¤‡æ¡ä»¶
    cond_input = content_latent
    uncond_input = torch.zeros_like(content_latent)  # ç©ºæ¡ä»¶ (ç”¨äºCFG)
    
    dt = 1.0 / steps
    
    # æ¬§æ‹‰ç§¯åˆ† + CFG
    for i in range(steps):
        t_current = torch.tensor([i * dt], device=device)
        
        # A. æœ‰æ¡ä»¶é¢„æµ‹ (çœ‹ç€å†…å®¹å›¾ç”»)
        v_cond = model(x_t, cond_input, t_current, style_tensor)
        
        # B. æ— æ¡ä»¶é¢„æµ‹ (ç›²ç”»)
        v_uncond = model(x_t, uncond_input, t_current, style_tensor)
        
        # C. CFG å¤–æ¨ (Extrapolation)
        # å…¬å¼: v_uncond + cfg_scale * (v_cond - v_uncond)
        # ä½œç”¨: æ”¾å¤§"å†…å®¹å›¾"å¸¦æ¥çš„ç‰¹å¾ï¼Œå¼ºåŠ›æŠ‘åˆ¶æ¨¡ç³Š
        v_final = v_uncond + cfg_scale * (v_cond - v_uncond)
        
        # æ›´æ–°ä½ç½®
        x_t = x_t + dt * v_final
    
    return x_t


@torch.no_grad()
def teleport_latent(content_latent, target_style_id, steps, model, device, noise_strength=1.0):
    """
    å…¼å®¹æ€§åŒ…è£…ï¼šè°ƒç”¨æ–°çš„ CFG é‡‡æ ·
    (ä¿ç•™æ—§æ¥å£ä»¥å…¼å®¹ç°æœ‰ä»£ç )
    """
    return generate_style_transfer_cfg(
        content_latent, 
        target_style_id, 
        model, 
        device, 
        steps=steps, 
        cfg_scale=4.0  # é»˜è®¤ CFG scale
    )


def load_vae_encoder_decoder():
    """åŠ è½½SD1.5çš„VAE"""
    from diffusers import AutoencoderKL
    vae = AutoencoderKL.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        subfolder="vae"
    )
    return vae


def image_to_latent(image_path, vae, device):
    """å°†å›¾ç‰‡ç¼–ç ä¸ºlatent"""
    from torchvision import transforms
    
    img = Image.open(image_path).convert("RGB")
    img = img.resize((512, 512))
    
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])
    ])
    
    img_tensor = transform(img).unsqueeze(0).to(device)
    
    vae = vae.to(device)
    with torch.no_grad():
        latent = vae.encode(img_tensor).latent_dist.sample()
        latent = latent * 0.18215
    
    return latent


def latent_to_image(latent, vae, device):
    """å°†latentè§£ç ä¸ºå›¾ç‰‡"""
    vae = vae.to(device)
    latent = latent / 0.18215
    
    with torch.no_grad():
        image = vae.decode(latent).sample
    
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
    image = (image * 255).astype(np.uint8)
    
    return Image.fromarray(image)


def find_checkpoint(checkpoint_dir, prefer="best"):
    checkpoint_dir = Path(checkpoint_dir)
    if prefer == "best":
        best_path = checkpoint_dir / "SAF_best.pt"
        if best_path.exists():
            return best_path
    
    checkpoints = list(checkpoint_dir.glob("SAF_epoch*.pt"))
    if checkpoints:
        import re
        epoch_numbers = []
        for ckpt in checkpoints:
            match = re.search(r'epoch(\d+)', ckpt.name)
            if match:
                epoch_numbers.append((int(match.group(1)), ckpt))
        
        if epoch_numbers:
            latest = max(epoch_numbers, key=lambda x: x[0])
            return latest[1]
    return None


def load_model_from_checkpoint(checkpoint_path, model, device):
    print(f"Loading checkpoint from {checkpoint_path}...")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        epoch = checkpoint.get('epoch', 'unknown')
        loss = checkpoint.get('loss', 'unknown')
        print(f"Loaded checkpoint from epoch {epoch} with loss {loss}")
    else:
        model.load_state_dict(checkpoint)
        print("Loaded model state dict")
    
    return model


def main():
    # ========== åŠ è½½é…ç½® ==========
    config = Config("config.json")
    
    model_cfg = config.model
    inf_cfg = config.inference
    ckpt_cfg = config.checkpoint
    
    # ========== æ¨ç†å‚æ•° ==========
    CHECKPOINT_PATH = "auto"
    INPUT_IMAGE = "test.jpg"
    TARGET_STYLE_ID = 1
    STEPS = inf_cfg.get('steps', 25)
    CFG_SCALE = inf_cfg.get('cfg_scale', 4.0)  # ğŸ”´ æ–°å¢ CFG å‚æ•°
    OUTPUT_ROOT = "inference_results"
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ========== åŠ è½½æ¨¡å‹ ==========
    print("Loading SA-Flow v2 model...")
    model = SAFModel(**model_cfg).to(device)
    
    if CHECKPOINT_PATH == "auto":
        checkpoint_dir = Path(ckpt_cfg['save_dir'])
        checkpoint_path = find_checkpoint(checkpoint_dir, prefer="best")
        if checkpoint_path is None:
            raise FileNotFoundError(f"No checkpoint found in '{checkpoint_dir}' directory")
    else:
        checkpoint_path = Path(CHECKPOINT_PATH)
    
    model = load_model_from_checkpoint(checkpoint_path, model, device)
    model.eval()
    
    # ========== åŠ è½½VAE ==========
    print("Loading VAE...")
    vae = load_vae_encoder_decoder()
    
    # ========== æ‰§è¡Œæ¨ç† ==========
    if not Path(INPUT_IMAGE).exists():
        print(f"âš ï¸ Input image {INPUT_IMAGE} not found. Please place a test image.")
        return

    print("Encoding input image...")
    content_latent = image_to_latent(INPUT_IMAGE, vae, device)
    
    input_base = Path(INPUT_IMAGE).stem
    style_str = f"style_{TARGET_STYLE_ID}"
    subdir = Path(OUTPUT_ROOT) / f"{input_base}_{style_str}_cfg{CFG_SCALE}"
    subdir.mkdir(parents=True, exist_ok=True)
    output_image_path = subdir / "output.jpg"
    
    print(f"Transferring to style {TARGET_STYLE_ID} with {STEPS} steps (CFG={CFG_SCALE})...")
    stylized_latent = generate_style_transfer_cfg(
        content_latent,
        TARGET_STYLE_ID,
        model,
        device,
        steps=STEPS,
        cfg_scale=CFG_SCALE
    )
    
    print("Decoding output image...")
    output_image = latent_to_image(stylized_latent, vae, device)
    output_image.save(output_image_path)
    print(f"âœ… Saved result to {output_image_path}")
    print(f"ğŸ’¡ Tip: Try different CFG scales (3.0, 5.0, 7.0) for sharpness control")


if __name__ == "__main__":
    main()