# LGT Quick Start Guide

## 1. Installation (2 minutes)

```bash
cd LGT
pip install -r requirements.txt
```

**Verify installation:**
```bash
python test_lgt.py
```

Expected output: `âœ“ All tests passed!`

---

## 2. Data Preparation (10-30 minutes)

### Organize your images:

```
raw_data/
  styleA/
    img001.jpg
    img002.jpg
    ...
  styleB/
    img003.jpg
    img004.jpg
    ...
```

### Pre-encode to VAE latents:

```bash
python preprocess_latents.py \
    --raw_data_root raw_data \
    --output_root data/latents \
    --target_size 256
```

**Output:**
```
data/latents/
  styleA/
    img001.pt  # [4, 32, 32] tensor
    img002.pt
  styleB/
    img003.pt
    img004.pt
```

### Update config.json:

```json
{
  "data": {
    "data_root": "data/latents",
    "style_subdirs": ["styleA", "styleB"]
  },
  "model": {
    "num_styles": 2
  }
}
```

---

## 3. Training (2-8 hours)

```bash
python train.py
```

**What happens:**
- Model trains for 100 epochs (configurable)
- Checkpoints saved every 5 epochs â†’ `checkpoints/epoch_XXXX.pt`
- Logs saved to `checkpoints/logs/training_*.csv`

**Monitor progress:**
```bash
# Watch log file (PowerShell)
Get-Content checkpoints/logs/training_*.csv -Wait -Tail 10

# Or open in Excel/spreadsheet for visualization
```

**Typical output:**
```
Epoch 1/100  | Loss: 0.8234 | SWD: 0.4521 | SSM: 0.3713 | LR: 1.00e-04
Epoch 2/100  | Loss: 0.7012 | SWD: 0.3891 | SSM: 0.3121 | LR: 9.98e-05
...
Epoch 100/100 | Loss: 0.2145 | SWD: 0.1023 | SSM: 0.1122 | LR: 1.00e-06
```

**Early stopping:** Look for SWD < 0.15 and SSM < 0.15 (good convergence)

---

## 4. Inference (< 1 minute per image)

### Python API:

```python
from inference import LGTInference, load_vae, encode_image, decode_latent
from PIL import Image
import torch
import numpy as np

# Setup (one-time)
device = 'cuda'
vae = load_vae(device)
lgt = LGTInference('checkpoints/epoch_0100.pt', device=device)

# Load image
img = Image.open('input.jpg').resize((256, 256))
img_tensor = torch.from_numpy(np.array(img)).float() / 255.0
img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0) * 2 - 1

# Encode â†’ Transfer â†’ Decode
latent_src = encode_image(vae, img_tensor, device)
latent_out = lgt.transfer_style(latent_src, source_style_id=0, target_style_id=1)
img_out = decode_latent(vae, latent_out, device)

# Save
from torchvision.utils import save_image
save_image(img_out, 'output.jpg')
```

### Command Line:

```bash
python inference.py \
    checkpoints/epoch_0100.pt \
    input.jpg \
    output.jpg \
    --target_style_id 1
```

---

## 5. Tuning for Best Results

### If style is weak:
```json
// In config.json
"loss": {
    "w_style": 2.0,  // Increase from 1.0
    "w_content": 1.0
}
```

### If structure is distorted:
```json
"loss": {
    "w_style": 1.0,
    "w_content": 2.0  // Increase from 1.0
}
```

### If textures are too smooth:
```json
"inference": {
    "temperature_lambda": 0.15  // Increase from 0.1
}
```

### If textures are too noisy:
```json
"inference": {
    "temperature_lambda": 0.05  // Decrease from 0.1
}
```

### For faster training (lower quality):
```json
"training": {
    "batch_size": 32,           // Larger batch
    "ode_integration_steps": 3  // Fewer ODE steps
}
```

### For higher quality (slower):
```json
"training": {
    "batch_size": 8,            // Smaller batch
    "ode_integration_steps": 10 // More ODE steps
},
"inference": {
    "num_steps": 50,            // More integration steps
    "cfg_scale": 3.0            // Stronger guidance
}
```

---

## 6. Troubleshooting

### Out of Memory (OOM)

**Error:** `CUDA out of memory`

**Solutions:**
```json
// Reduce batch size
"training": {
    "batch_size": 8  // or 4
}

// Reduce SWD samples
"loss": {
    "max_samples": 2048
}

// Reduce model size
"model": {
    "base_channels": 96  // from 128
}
```

### Training not converging

**Symptom:** Loss stays high after 50+ epochs

**Solutions:**
1. Check data quality: Are there enough samples? (Recommend 500+ per style)
2. Lower learning rate: `"learning_rate": 5e-5`
3. Increase content weight initially: `"w_content": 2.0`

### Poor style transfer quality

**Symptom:** Output doesn't match target style

**Solutions:**
1. Train longer: 150-200 epochs instead of 100
2. Increase style weight: `"w_style": 2.0`
3. More inference steps: `"num_steps": 30`
4. Higher temperature: `"temperature_lambda": 0.15`

### Structure loss during transfer

**Symptom:** Content structure is damaged

**Solutions:**
1. Increase content weight: `"w_content": 2.0` or `3.0`
2. Lower temperature: `"temperature_lambda": 0.05`
3. Lower CFG: `"cfg_scale": 1.5`

---

## 7. Example Workflow

### Full pipeline (horse â†’ zebra style transfer):

```bash
# 1. Prepare data
python preprocess_latents.py \
    --raw_data_root dataset/horse2zebra \
    --output_root data/latents_h2z

# 2. Update config
# Edit config.json:
#   "data_root": "data/latents_h2z"
#   "style_subdirs": ["trainA", "trainB"]

# 3. Train
python train.py

# 4. Test single image
python inference.py \
    checkpoints/epoch_0100.pt \
    test_horse.jpg \
    test_zebra.jpg \
    --target_style_id 1

# 5. Batch inference (create script)
python batch_inference.py --input_dir test_images --output_dir results
```

### Batch inference script (save as `batch_inference.py`):

```python
from inference import LGTInference, load_vae, encode_image, decode_latent
from PIL import Image
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm

device = 'cuda'
vae = load_vae(device)
lgt = LGTInference('checkpoints/epoch_0100.pt', device=device)

input_dir = Path('test_images')
output_dir = Path('results')
output_dir.mkdir(exist_ok=True)

for img_path in tqdm(list(input_dir.glob('*.jpg'))):
    # Load
    img = Image.open(img_path).resize((256, 256))
    img_tensor = torch.from_numpy(np.array(img)).float() / 255.0
    img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0) * 2 - 1
    
    # Process
    latent_src = encode_image(vae, img_tensor, device)
    latent_out = lgt.transfer_style(latent_src, 0, 1)
    img_out = decode_latent(vae, latent_out, device)
    
    # Save
    from torchvision.utils import save_image
    save_image(img_out, output_dir / img_path.name)
```

---

## 8. Expected Timeline

| Task | Time | GPU Usage |
|------|------|-----------|
| Installation | 2 min | 0% |
| Data preprocessing (1000 images) | 15 min | 50-70% |
| Training (100 epochs, 2000 samples) | 4-6 hours | 90-100% |
| Inference (per image) | 10-30 sec | 50-80% |

**Total project time:** ~6 hours (mostly hands-off training)

---

## 9. Hardware Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| GPU | RTX 3060 (12GB) | RTX 4070+ (8GB+) |
| RAM | 16GB | 32GB |
| Storage | 10GB free | 50GB free |
| OS | Windows 10/11, Linux | Any |

**Performance on RTX 4070 Laptop:**
- Training: ~1.5 sec/epoch (batch_size=16, 2000 samples)
- Inference: ~15 sec/image (num_steps=20)
- VRAM usage: ~6.5GB peak

---

## 10. Next Steps

After basic training works:

1. **Improve quality:**
   - Train longer (200 epochs)
   - Collect more data (2000+ images per style)
   - Tune energy weights (w_style, w_content)

2. **Add more styles:**
   - Update `num_styles` in config
   - Add more style folders
   - Train multi-style model

3. **Optimize speed:**
   - Enable `torch.compile`
   - Reduce inference steps
   - Try FP16 instead of BFloat16

4. **Advanced features:**
   - Style interpolation
   - Multi-resolution training
   - Custom loss terms (see API.md)

---

## Need Help?

- Check [README.md](README.md) for detailed documentation
- Check [API.md](API.md) for programming reference
- Run `python test_lgt.py` to verify installation
- Open GitHub issue with error logs

**Happy transferring! ðŸŽ¨**
